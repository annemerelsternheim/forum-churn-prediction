{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# python <file>.py <topics>.json <posts>.json <path-for-csv-files>\n",
    "\n",
    "# optional: adding one or more user-IDs. This enables you to run the program for just a few users, instead of the entire batch.\n",
    "# non-existing user-IDs will not raise an error, they are simply ignored\n",
    "\n",
    "# command line example:\n",
    "# python 2018_03_29_variables_to_csv.py \"D:\\topics.json\" \"D:\\posts.json\" C:\\user-csvs 1144,1433\n",
    "\n",
    "# a successful run will within a few seconds show a progress bar (which takes 40 seconds to complete, on my computer): this progress bar visualises the colleciton of the data\n",
    "# then you will see a second progress bar, showing how many user IDs were selected to analyse data from, and an estimation of the total running time (at least after the first user is analysed)\n",
    "# a third progress bar shows how long the analysis for the first user is taking.\n",
    "# Then some info about this user's activity is printed\n",
    "# and then the next, and the next, and the next user...\n",
    "# a square is printed if the program has finished. For fun.\n",
    "\n",
    "# quick copy-paste for me:\n",
    "# python 2018_03_29_variables_to_csv.py \"D:\\4. Data\\Amazones_Forum_Export_JSON\\2017-12-07T13-36-51_amazones_forum_topics_export.json\" \"D:\\4. Data\\Amazones_Forum_Export_JSON\\2017-12-07T13-39-20_amazones_forum_posts_export.json\" \"C:\\Users\\sternheimam\\Desktop\\my-notebook\\user-csvs\" 1144\n",
    "\n",
    "############################################################\n",
    "## IMPORTS\n",
    "############################################################\n",
    "\n",
    "import json, re, time, unicodedata, unidecode, itertools, os, sys\n",
    "from collections import defaultdict\n",
    "from datetime import date, datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pattern.nl import sentiment\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "## FUNCTION DEFINITIONS\n",
    "############################################################\n",
    "\n",
    "#---------------------------\n",
    "# Prepare text in .json files for measurements etc\n",
    "#---------------------------\n",
    "def remove_non_ascii(text):\n",
    "\t\"\"\" this function expects a string, and removes non-ascii characters from it \"\"\"\n",
    "\treturn unidecode.unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    \"\"\" this function expects a string (post from the BVN/Amazones forum), and returns a cleaner version of it \"\"\"\n",
    "    # remove all links, images, quotes, and emailaddresses\n",
    "    text=re.sub('<a.*?>(.*?)</a>','',text) #remove links\n",
    "    text=re.sub('(http:|www)\\S*','',text) #remove links without markup\n",
    "    text=re.sub('\\[\\\\\\/url\\]','',text)\n",
    "    text=re.sub('<img.*?/>', '',text) #remove images\n",
    "    text=re.sub('<div class=\"bb-quote\">((\\s|\\S)*?)</div>','',text) #remove quotes\n",
    "    text=re.sub('<script.*?>([\\S\\s]*?)</script>','',text) #remove emailaddresses\n",
    "\n",
    "    # replace all emoticon-icons\n",
    "    text=re.sub('<img.*?title=\"(.*?)\".*?/>', '(EMO:\\\\1)',text) #replace emoticons by textual indicators \n",
    "\n",
    "    # replace (most) sideways latin emoticons\n",
    "    text=re.sub('[^>]:-?(\\)|\\])','(EMO:smiley)',text)\n",
    "    text=re.sub(u'☺️','(EMO:smiley)',text)\n",
    "    text=re.sub('[^>]:-?(\\(|\\[)','(EMO:sad)',text)\n",
    "    text=re.sub(';-?(\\)|\\])','(EMO:wink)',text)\n",
    "    text=re.sub(r'(:|;|x|X)-?(D)+\\b','(EMO:laugh)',text)\n",
    "    text=re.sub(':-?(/|\\\\\\|\\|)','(EMO:frown)',text)\n",
    "    text=re.sub(r'(:|;)-?(p|P)+\\b','(EMO:cheeky)',text)\n",
    "    text=re.sub('(:|;)(\\'|\\\")-?(\\(|\\[)','(EMO:cry)',text)\n",
    "    text=re.sub('\\<3+','(EMO:heart)',text)\n",
    "    text=re.sub(u'❤️','(EMO:heart)',text)\n",
    "    text=re.sub('((\\>:-?(\\(|\\]))|(\\>?:-?@))','(EMO:angry)',text)\n",
    "    text=re.sub('\\>:-?(\\)|\\])','(EMO:evil)',text)\n",
    "    text=re.sub(r'(:|;)-?(O|o|0)+\\b','(EMO:shock)',text)\n",
    "    text=re.sub('(:|;)-?(K|k|x|X)','(EMO:kiss)',text)\n",
    "    # :s\n",
    "    # :x is eigenlijk geen kus, geloof ik...\n",
    "\n",
    "    #other important adjustments:\n",
    "    text=re.sub('m\\'?n\\s','mijn ',text) # replacing m'n and mn with mijn, so it gets parsed correctly.\n",
    "    text=re.sub('z\\'?n\\s','zijn ',text) #replacing z'n and zn with zijn\n",
    "    text=re.sub('d\\'?r\\s','haar ',text) #replacing d'r and dr with zijn (only if followed by space, so dr. stays dr.)\n",
    "\n",
    "    # replace all emoticons (and other things) written between double colons\n",
    "    text=re.sub(':([a-zA-Z]+):','(EMO:\\\\1)',text)\n",
    "\n",
    "    # remove remaining markup\n",
    "    text=re.sub('</?(ol|style|b|p|em|u|i|strong|br|span|div|blockquote|li)(.*?)/?>','',text)\n",
    "    text=re.sub('(\\[|\\]|\\{|\\})', '',text)\n",
    "\n",
    "    # separate text from punctuation (may cause double/triple spaces - does not matter at this point)\n",
    "    text = re.sub('(\\.{2,}|/|\\)|,|!|\\?)','\\\\1 ',text) # space behind\n",
    "    text = re.sub('(/|\\()',' \\\\1',text) # space in front\n",
    "    text = re.sub('(\\w{2,})(\\.|,)','\\\\1 \\\\2 ',text) #space 'between'\n",
    "\n",
    "    return(remove_non_ascii(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_P_T_and_D(topics1,topics2,posts1,posts2):\n",
    "    \"\"\" this function takes the .json files containing the thread starts and responses, and returns three things:\n",
    "    [0]: a dictionary with the user-ID as key, and the post as value;\n",
    "    [1]: a dictionary with the user-ID as key, and the time of posting as a value;\n",
    "    [2]: a list of all datetimes present in the data (sorted by date, because the .json was already sorted) \"\"\"\n",
    "    P = defaultdict(list)\n",
    "    T = defaultdict(list)\n",
    "    D = []\n",
    "\n",
    "    print \"Loading ...\"\n",
    "    with tqdm(total=len(topics1)+len(topics2)+len(posts1)+len(posts2)) as pbar:\n",
    "        for t1 in reversed(topics1):\n",
    "            pbar.update(1)\n",
    "            P[t1['Author uid']].append((cleanup(t1[\"Body\"]),1))\n",
    "            T[t1['Author uid']].append(t1['Post date'])\n",
    "            D.append(datetime.strptime(t1['Post date'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "        for t2 in reversed(topics2):\n",
    "            pbar.update(1)\n",
    "            P[t2['user_id']].append((cleanup(t2[\"body\"]),1))\n",
    "            T[t2['user_id']].append(t2['post_date'])\n",
    "            D.append(datetime.strptime(t2['post_date'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "        for p1 in reversed(posts1):\n",
    "            pbar.update(1)\n",
    "            P[p1['Auteur-uid']].append((cleanup(p1[\"Body\"]),0))\n",
    "            T[p1['Auteur-uid']].append(p1['Datum van inzending'])\n",
    "            D.append(datetime.strptime(p1['Datum van inzending'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "        for p2 in reversed(posts2):\n",
    "            pbar.update(1)\n",
    "            P[p2['user_id']].append((cleanup(p2[\"body\"]),0))\n",
    "            T[p2['user_id']].append(p2['post_date'])\n",
    "            D.append(datetime.strptime(p2['post_date'], '%d/%m/%Y - %H:%M'))\n",
    "    print \"Loading complete! Now loop through all users, and make csv files: \"\n",
    "    return (P,T,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# Make list of bin-bondaries-to-be (binlist)\n",
    "#---------------------------\n",
    "def make_binlist(D,hours = None, days = None, months = None):\n",
    "    \"\"\" this function takes a list of dates (D), and generates a new list of dates,\n",
    "    starting at 4:00 AM just before the earliest date in D, and ending at 4:00 just after the latest date in D,\n",
    "    with fixed timeticks between all dates in the list.\n",
    "    Optionally, the length of the timetick may be specified (in hours).\n",
    "    \"\"\"\n",
    "    #set lower and upper boundaries of a user's activity\n",
    "    lower = min(D)\n",
    "    upper = max(D)\n",
    "\n",
    "    if lower.time()>=datetime.strptime('4:00','%H:%M').time():\n",
    "        lower = lower.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        lower = (lower+timedelta(days = -1)).replace(hour=4,minute=0)\n",
    "\n",
    "    if upper.time()<datetime.strptime('12:00','%H:%M').time():\n",
    "        upper = upper.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        upper = (upper+timedelta(days=1)).replace(hour=4,minute=0)\n",
    "\n",
    "    # als er uren ingegeven zijn, neem dan een zoveel-uur-bin\n",
    "    if hours != None:\n",
    "        return([lower + timedelta(hours=x) for x in range(0, 24*((upper-lower).days), hours)])\n",
    "    # als er dagen ingegeven zijn, neem dan een zoveel-dag-bin\n",
    "    elif days != None:\n",
    "        return([lower + timedelta(days=x) for x in range(0, (upper-lower).days, days)])\n",
    "    # als er maanden ingegeven zijn, neem dan een zoveel-maand-bin\n",
    "    elif months != None:\n",
    "        return([lower + relativedelta(months=x) for x in range(0, ((upper-lower).days)/30, months)])\n",
    "    else:\n",
    "        print \"please specify the size of the bins in hours, days or months\"\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# List all created dates / first accesses\n",
    "#---------------------------\n",
    "def make_first_access_dict(first_access_dict = dict()):\n",
    "    for u in userstatus:\n",
    "        first_access_dict[u['user_id']] = u['created_date']\n",
    "    return first_access_dict\n",
    "\n",
    "#---------------------------\n",
    "# List all last accesses (for both blocked and active users)\n",
    "#---------------------------\n",
    "def make_last_access_dict(last_access_dict = dict()):\n",
    "    for u in userstatus:\n",
    "        last_access_dict[u['user_id']] = u['last_access']\n",
    "    return last_access_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# User selection\n",
    "#---------------------------\n",
    "def determine_active_users(include_only = []):\n",
    "    global over_treshold\n",
    "\n",
    "    E = open('exclude.txt', 'r')\n",
    "    lines = E.readlines()\n",
    "    exclude = [l.rstrip() for l in lines]\n",
    "\n",
    "    # if the function is given an argument, work with that specific data instead of all users\n",
    "    if include_only == []:\n",
    "        userlist = [u['user_id'] for u in userstatus]\n",
    "    else:\n",
    "        userlist = include_only\n",
    "\n",
    "    # only consider users with a minimum of 30 posts, and those whose data was not obviously messed up/with\n",
    "    for user in userlist:\n",
    "        if len(T[user])<30:\n",
    "            pass\n",
    "        elif user in exclude:\n",
    "            pass\n",
    "        else:\n",
    "            over_treshold.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# Variable annotation\n",
    "#---------------------------\n",
    "def determine_questionmarks(body, Q=0):\n",
    "    \"\"\" This function counts and returns the number of sentences in the provided input string\n",
    "    that ends in at least one question mark \"\"\"\n",
    "    for sentence in sent_tokenize(body):\n",
    "        if re.search('\\?+', sentence):\n",
    "            Q+=1\n",
    "    if len(sent_tokenize(body))!=0:\n",
    "        return float(Q)/float(len(sent_tokenize(body)))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def determine_sentiment(body):\n",
    "    \"\"\" this funciton determines and returns the average sentiment of sentences in the provided input string.\n",
    "    It uses the pattern module to do so. Sentiment values may range from -1 to 1. \"\"\"\n",
    "    return np.mean([sentiment(sentence)[0] for sentence in sent_tokenize(body)]) \n",
    "\n",
    "def determine_subjectivity(body):\n",
    "    \"\"\" This function determines and returns the average subjectivity of sentences in the provided input string.\n",
    "    It uses the pattern module to do so. Subjectivity values may range from 0 to 1. \"\"\"\n",
    "    return np.mean([sentiment(sentence)[1] for sentence in sent_tokenize(body)]) \n",
    "\n",
    "def determine_post_length(body):\n",
    "    \"\"\" This function determines and returns the length of the provided input string in sentences.\n",
    "    It uses the nltk sent_tokenize function to do so. \"\"\"\n",
    "    return(len(sent_tokenize(body)))\n",
    "\n",
    "def determine_sentence_length(body):\n",
    "    \"\"\" This function determines and returns the average length of the sentences in the provided input string in words.\n",
    "    It uses the nltk word_tokenize function to do so. \"\"\"\n",
    "    #word_tokenize also considers interpunction a word\n",
    "    return np.mean([len(word_tokenize(sentence)) for sentence in sent_tokenize(body)])\n",
    "\n",
    "def determine_week_activity(first_date,last_date,bindict):\n",
    "    \"\"\" This function returns a dictionary that has kept track of the activity in week-bins, instead of day-bins.\n",
    "    It expects two dates, to indicate in between which dates the dictionary should be built,\n",
    "    and expects a dictionary in which all the user's active times are already stored\"\"\"\n",
    "    for d in range(0, (last_date-first_date).days,7):\n",
    "        week_start = first_date+timedelta(days=d)\n",
    "        week_end = first_date+timedelta(days=d+7)\n",
    "        for date in list(itertools.chain.from_iterable(bindict.values())):\n",
    "            if week_start<=datetime.strptime(date, '%d/%m/%Y - %H:%M')<week_end:\n",
    "                weekcountdict[week_start,week_end].append(1)\n",
    "        if len(weekcountdict[week_start,week_end])==0:\n",
    "            weekcountdict[week_start,week_end] = []\n",
    "    return weekcountdict\n",
    "\n",
    "def determine_past_activity(bindict,index,hours_back=24):\n",
    "    \"\"\" This function returns the number of times a user has been active in the last hours_back hours (default 24h).\n",
    "    The first hours_back time bins will for now have a value of 0 by default, to keep things easy.\"\"\"\n",
    "    if 0<= index-hours_back<=len(bindict):\n",
    "        past_activity = np.sum([len(bindict[binlist[index-(x+1)],binlist[index+1-(x+1)]]) for x in range(hours_back)])\n",
    "    else:\n",
    "        past_activity=0\n",
    "    return past_activity\n",
    "\n",
    "def print_information(user, T, first_date,last_date,weekcount):\n",
    "    \"\"\" this function prints useful basic information on the user's activity.\n",
    "    It needs quite some input so make sure you've got them all:\n",
    "    1) user ID, 2) a dictionary containing users-IDs as key, and any activity log as value,\n",
    "    3,4) the first and last date of activity, and 5) the dictionary that kept track of the week activity. \"\"\"\n",
    "    print \"User:\", user\n",
    "    print \"posted one or more posts in\", len(T[user]), \"'bins'.\" \n",
    "    print \"The first post: \", first_date\n",
    "    print \"The last post: \", last_date\n",
    "    print \"Activity spread over: \", last_date-first_date\n",
    "    print \"The average nr of posts per week: \", np.mean([len(x) for x in weekcount.values()]), \"including long times of inactivity.\"\n",
    "    print \"The average nr of posts in non-empty weeks: \", np.mean([len(x) for x in weekcount.values() if not x==[]])\n",
    "    print \"The range of activity: \", min([len(x) for x in weekcount.values()]), \" to \", max([len(x) for x in weekcount.values()]), \" posts per week\"\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# Kneading df into the right shape\n",
    "#---------------------------\n",
    "def swap_to_front(header, dependent_variable):\n",
    "    \"\"\" this function takes a list of column names, and the column name that is to be placed in front. It places this column name on the [1]th position of the list of column names. \"\"\"\n",
    "    header.remove(dependent_variable)\n",
    "    header.insert(0,dependent_variable)\n",
    "    return header\n",
    "\n",
    "def place_datetime_first(df,dependent_variable=\"Date & Time\"):\n",
    "    \"\"\" this function takes a dataframe, and the name of the column that is to be switched to the front. It places this column on the [1]th position in the dataframe. \"\"\"\n",
    "    header = df.columns.tolist()\n",
    "    header = swap_to_front(header,dependent_variable)\n",
    "    new_df = df.reindex(columns = header)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "## MAIN CODE\n",
    "############################################################\n",
    "\n",
    "#---------------------------\n",
    "# load the files\n",
    "#---------------------------\n",
    "topics1 = json.load(open('D:\\\\4. Data\\\\Amazones\\\\oud_topics.json'))\n",
    "topics2 = json.load(open('D:\\\\4. Data\\\\Amazones\\\\nieuw_topics.json'))\n",
    "posts1 = json.load(open('D:\\\\4. Data\\\\Amazones\\\\oud_posts.json'))\n",
    "posts2 = json.load(open('D:\\\\4. Data\\\\Amazones\\\\nieuw_posts.json'))\n",
    "users1 = json.load(open('D:\\\\4. Data\\\\Amazones\\\\oud_users.json'))\n",
    "users2 = json.load(open('D:\\\\4. Data\\\\Amazones\\\\nieuw_users.json'))\n",
    "userstatus = json.load(open('D:\\\\4. Data\\\\Amazones\\\\nieuw_users_status.json'))\n",
    "\n",
    "#---------------------------\n",
    "# sort out the data per user (values are either Posts or Times in lists), and make a list of all datetimes in the data\n",
    "#---------------------------\n",
    "P,T,D = make_P_T_and_D(topics1,topics2,posts1,posts2)\n",
    "\n",
    "#---------------------------\n",
    "# determine 'relevant' users\n",
    "#---------------------------\n",
    "# initiate empty list of users relevant to measure\n",
    "over_treshold = []\n",
    "determine_active_users(['926']) # determines all users that have posted at least 30 posts, from a specified list or (default) from all users in the userstatus file.\n",
    "\n",
    "#---------------------------\n",
    "# global variables\n",
    "#---------------------------\n",
    "# path for saving csv files\n",
    "path = r\"C:\\Users\\sternheimam\\Desktop\\my-notebook\\user-csvs\"\n",
    "\n",
    "# chronological list of (lower) bin boundaries\n",
    "binlist = make_binlist(D,days=1)\n",
    "first_access_dict = make_first_access_dict()\n",
    "last_access_dict = make_last_access_dict()\n",
    "\n",
    "# The negative difference for measuring the 'backtrack' feature\n",
    "neg_diff = 24\n",
    "\n",
    "#---------------------------\n",
    "# go through all users in over_treshold, and do stuff..\n",
    "#---------------------------\n",
    "# show the progress, while going through the active users\n",
    "with tqdm(total=len(over_treshold)) as processbar:\n",
    "    for user in over_treshold:\n",
    "        print user,\n",
    "        processbar.update(1)\n",
    "\n",
    "        #---------------------------\n",
    "        # initiate some user-specific variables\n",
    "        #---------------------------\n",
    "        first_access = datetime.strptime(first_access_dict[user], '%d/%m/%Y - %H:%M')\n",
    "        last_access = datetime.strptime(last_access_dict[user], '%d/%m/%Y - %H:%M')\n",
    "        first_date = 0\n",
    "        last_date = 0\n",
    "        inactivity = 0 # change to first access date, then re-calculate first inactivity with this date\n",
    "\n",
    "        # all lists starting with csv_ are lists that will eventually contain all values that end up in the csv file\n",
    "        csv_date = []\t\t\t#datetime\n",
    "        csv_sentiment = []\t   #sentiment value (-1 to 1)\n",
    "        csv_questionmarks = []   #question mark-ending sentences (float)\n",
    "        csv_subjectivity = []\t#subjectivity value (0 to 1)\n",
    "        csv_sentencelength = []  #length of sentence in words (float)\n",
    "        csv_postlength = []\t  #length of post in sentences (float)\n",
    "        csv_startposts = []\t  #1 for a thread start, 0 for a response (float) \n",
    "        csv_inactivity = []\t  #hours passed since last activity (int)\n",
    "        csv_backtrack = []\t   #posts posted in last x hours (x = neg_diff; default 24h)\n",
    "        csv_churn_year = []\n",
    "        csv_churn_6months = []\n",
    "        csv_churn_3months = []\n",
    "\n",
    "        # dictionaries to keep track of activity within certain time bins\n",
    "        bindict = defaultdict(list)\t\t#bins with size 'timetick' (default 1h), values = post-times \n",
    "        postdict = defaultdict(list)\t   #bins with size 'timetick' (default 1h), values = posts\n",
    "        metadict = defaultdict(list)\t   #bins with size 'timetick' (default 1h), values = 1 or 0 (start or response)\n",
    "        weekcountdict = defaultdict(list)  #bins with size = 7 days, values = '1' for every post\n",
    "        backtrackdict = defaultdict(list)  #bins with size = neg_diff (default 24h), values = nr of posts in last neg_diff hours\n",
    "        churn_yeardict = defaultdict(list)\n",
    "        churn_6monthsdict = defaultdict(list)\n",
    "        churn_3monthsdict = defaultdict(list)\n",
    "                # TO DO: linguistic markers, like adjectives / pronouns / emoticons, and the diversity of topics / vocabulary\n",
    "\n",
    "        #---------------------------\n",
    "        # loop through the (sorted) list of datetimes, and do stuff..\n",
    "        #---------------------------\n",
    "        for index,boundary in enumerate(tqdm(binlist)):\n",
    "            # determine time bin boundaries for dictionaries\n",
    "            if index+1>=len(binlist):\n",
    "                break\n",
    "            else:\n",
    "                lower = binlist[index]\n",
    "                upper = binlist[index+1]\n",
    "\n",
    "                #---------------------------\n",
    "                # Loop through T, collecting all activity for the selected user\n",
    "                #---------------------------\n",
    "                # determine in which time bin the user's activity belongs\n",
    "                for time in T[user]: # T contains more users than userstatus, but that does not matter because the users not in userstatus are simply ignored in T.\n",
    "                    if lower<=datetime.strptime(time, '%d/%m/%Y - %H:%M')<upper:\n",
    "                        bindict[lower,upper].append(time)\n",
    "\n",
    "                        # and determine how active the user has been in past neg_diff hours\n",
    "                        past_activity = determine_past_activity(bindict,index,neg_diff)\n",
    "                        backtrackdict[lower,upper].append(past_activity)\n",
    "\n",
    "                        # determine the first and last active dates\n",
    "                        if first_date == 0:\n",
    "                            first_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                            last_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                        else:\n",
    "                            last_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "\n",
    "                        # split the text in P from the start/response-information\n",
    "                        body = P[user][T[user].index(time)][0]\n",
    "                        meta = P[user][T[user].index(time)][1]\n",
    "                        postdict[lower,upper].append(body) \n",
    "                        metadict[lower,upper].append(meta)\n",
    "\n",
    "                        # determine whether the patient will churn within one year/6 months/3 months\n",
    "                        churn_yeardict[lower,upper].append(1 if (datetime.strptime(time, '%d/%m/%Y - %H:%M')+relativedelta(months=12))>last_access else 0)\n",
    "                        churn_6monthsdict[lower,upper].append(1 if (datetime.strptime(time, '%d/%m/%Y - %H:%M')+relativedelta(months=6))>last_access else 0)\n",
    "                        churn_3monthsdict[lower,upper].append(1 if (datetime.strptime(time, '%d/%m/%Y - %H:%M')+relativedelta(months=3))>last_access else 0)\n",
    "\n",
    "                # fill up the still-empty places in the dictionary\n",
    "                if len(bindict[lower,upper])==0:\n",
    "                    bindict[lower,upper]=[]\n",
    "                    postdict[lower,upper]=[]\n",
    "                    metadict[lower,upper]=[]\n",
    "                    backtrackdict[lower,upper]=[]\n",
    "\n",
    "\n",
    "                #---------------------------\n",
    "                # Fill csv_feature-lists with values \n",
    "                #---------------------------\t\t\t\t\n",
    "                # Treat different posts within same bin 'as one' (concatenate them)\n",
    "                body = '. '.join(postdict[lower,upper]) #can be empty!\n",
    "\n",
    "                #when the bin is before the first access date, you don't do anything. Inactivity is kept 0 until bins are 'within' the user's frame\n",
    "                if datetime.strptime(time, '%d/%m/%Y - %H:%M')<first_access:\n",
    "                    inactivity = 0\n",
    "                # when the bin is after last acces AND the bin is empty, add 1 to the inactivity feature (this keeps track of how many bins were between last activity and current)\n",
    "                elif len(body) == 0:\n",
    "                    inactivity+=1\n",
    "                # when the bin is after last access AND NOT empty, append a value to all csv_feature-lists (and reset 'inactivity')\n",
    "                else:\t\n",
    "                    csv_date.append(lower)\n",
    "                    csv_sentiment.append(determine_sentiment(body))\n",
    "                    csv_questionmarks.append(determine_questionmarks(body))\n",
    "                    csv_subjectivity.append(determine_subjectivity(body))\n",
    "                    csv_sentencelength.append(determine_sentence_length(body))\n",
    "                    csv_postlength.append(np.mean([determine_post_length(x) for x in postdict[lower,upper]]))\n",
    "                    csv_startposts.append(np.mean(metadict[lower,upper]))\n",
    "                    csv_inactivity.append(inactivity)\n",
    "                    csv_backtrack.append(np.sum(backtrackdict[lower,upper][-1]))\n",
    "                    csv_churn_year.append(1 if np.mean(churn_yeardict[lower,upper])>=0.5 else 0)\n",
    "                    csv_churn_6months.append(1 if np.mean(churn_6monthsdict[lower,upper])>=0.5 else 0)\n",
    "                    csv_churn_3months.append(1 if np.mean(churn_3monthsdict[lower,upper])>=0.5 else 0)\n",
    "                    inactivity = 0\n",
    "\n",
    "        #---------------------------\n",
    "        # Report the results for this user\n",
    "        #--------------------------- \n",
    "        # determine average activity: over active period, and over only-active weeks \n",
    "        weekcount = determine_week_activity(first_date,last_date,bindict)   \n",
    "        print_information(user, T, first_date,last_date,weekcount)\t\t\t\n",
    "\n",
    "        # put all csv_features into a dataframe\n",
    "        df = pd.DataFrame({\"Date & Time\": csv_date, \"Sentiment\": csv_sentiment, \"Questions\": csv_questionmarks, \n",
    "                           \"Subjectivity\": csv_subjectivity, \"Words/Sentence\": csv_sentencelength, \n",
    "                           \"Sentences/Post\": csv_postlength, \"First posts\": csv_startposts,\n",
    "                           \"Inactivity\": csv_inactivity, \"Posts in last 24H\": csv_backtrack},\n",
    "                           \"Churn in y\": csv_churn_year,\"Churn in 6m\": csv_churn_6months,\"Churn in 3m\": csv_churn_3months).dropna()\n",
    "        df = place_datetime_first(df)\n",
    "        name = \"features_user_\"+str(user)+\".csv\"\t   \n",
    "        df.to_csv(os.path.join(path,name),index=False)\n",
    "print \" _ \"\n",
    "print \"|_|\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make forwards looking variables: they should summarize over some period in the future\n",
    "# make py so, that it returns two file versions: 'month bins' and 'day bins'. (perhaps also placed in different folders)\n",
    "# maak samengeraapte activiteitsvariabele\n",
    "# normaliseer variabelen\n",
    "# ga xgboosten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
