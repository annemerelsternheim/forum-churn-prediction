{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import seaborn as sbn\n",
    "import json, re, time, unicodedata, unidecode, codecs, random, math, warnings, itertools, os\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "from pattern.nl import parse, split, parsetree\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import date, datetime, timedelta\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pattern.nl import sentiment\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import normaltest\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# files\n",
    "#MWE\n",
    "#topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE_topic.json'))\n",
    "#posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE.json'))\n",
    "#regular\n",
    "forums = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-35-45 _amazones_forums_export.json'))\n",
    "# zorg voor een nieuwe versie van dit bestand; verkeerd opgeslagen dus je mist het kontje!\n",
    "topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-36-51_amazones_forum_topics_export.json'))\n",
    "posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_forum_posts_export.json'))\n",
    "users = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_users_export.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove non-ascii characters from file (otherwise they will become part of the tokens)\n",
    "def remove_non_ascii(text):\n",
    "    return unidecode.unidecode(text)\n",
    "    #return ''.join([i if ord(i) < 128 else ' ' for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    #remove all links, images, quotes, and emailaddresses\n",
    "    text=re.sub('<a.*?>(.*?)</a>','',text) #remove links\n",
    "    text=re.sub('(http:|www)\\S*','',text) #remove links without markup\n",
    "    text=re.sub('\\[\\\\\\/url\\]','',text)\n",
    "    text=re.sub('<img.*?/>', '',text) #remove images\n",
    "    text=re.sub('<div class=\"bb-quote\">((\\s|\\S)*?)</div>','',text) #remove quotes\n",
    "    text=re.sub('<script.*?>([\\S\\s]*?)</script>','',text) #remove emailaddresses\n",
    "\n",
    "    #replace all emoticon-icons\n",
    "    text=re.sub('<img.*?title=\"(.*?)\".*?/>', '(EMO:\\\\1)',text) #replace emoticons by textual indicators \n",
    "\n",
    "    # replace (most) sideways latin emoticons\n",
    "    text=re.sub('[^>]:-?(\\)|\\])','(EMO:smiley)',text)\n",
    "    text=re.sub(u'☺️','(EMO:smiley)',text)\n",
    "    text=re.sub('[^>]:-?(\\(|\\[)','(EMO:sad)',text)\n",
    "    text=re.sub(';-?(\\)|\\])','(EMO:wink)',text)\n",
    "    text=re.sub(r'(:|;|x|X)-?(D)+\\b','(EMO:laugh)',text)\n",
    "    text=re.sub(':-?(/|\\\\\\|\\|)','(EMO:frown)',text)\n",
    "    text=re.sub(r'(:|;)-?(p|P)+\\b','(EMO:cheeky)',text)\n",
    "    text=re.sub('(:|;)(\\'|\\\")-?(\\(|\\[)','(EMO:cry)',text)\n",
    "    text=re.sub('\\<3+','(EMO:heart)',text)\n",
    "    text=re.sub(u'❤️','(EMO:heart)',text)\n",
    "    text=re.sub('((\\>:-?(\\(|\\]))|(\\>?:-?@))','(EMO:angry)',text)\n",
    "    text=re.sub('\\>:-?(\\)|\\])','(EMO:evil)',text)\n",
    "    text=re.sub(r'(:|;)-?(O|o|0)+\\b','(EMO:shock)',text)\n",
    "    text=re.sub('(:|;)-?(K|k|x|X)','(EMO:kiss)',text)\n",
    "    # :s\n",
    "    # :x is eigenlijk geen kus, geloof ik...\n",
    "\n",
    "\n",
    "    #other important adjustments:\n",
    "    text=re.sub('m\\'?n\\s','mijn ',text) # replacing m'n and mn with mijn, so it gets parsed correctly.\n",
    "    text=re.sub('z\\'?n\\s','zijn ',text) #replacing z'n and zn with zijn\n",
    "    text=re.sub('d\\'?r\\s','haar ',text) #replacing d'r and dr with zijn (only if followed by space, so dr. stays dr.)\n",
    "\n",
    "    # replace all emoticons (and other things) written between double colons\n",
    "    text=re.sub(':([a-zA-Z]+):','(EMO:\\\\1)',text)\n",
    "\n",
    "    # remove remaining markup\n",
    "    text=re.sub('</?(ol|style|b|p|em|u|i|strong|br|span|div|blockquote|li)(.*?)/?>','',text)\n",
    "    text=re.sub('(\\[|\\]|\\{|\\})', '',text)\n",
    "\n",
    "    # separate text from punctuation (may cause double/triple spaces - does not matter at this point)\n",
    "    text = re.sub('(\\.{2,}|/|\\)|,|!|\\?)','\\\\1 ',text) # space behind\n",
    "    text=re.sub('(/|\\()',' \\\\1',text) # space in front\n",
    "    text=re.sub('(\\w{2,})(\\.|,)','\\\\1 \\\\2 ',text) #space 'between'\n",
    "\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make two dictionaries: user's post T imes, and user's P ost texts.\n",
    "def make_P_T_and_D(topics,posts):\n",
    "    P = defaultdict(list)\n",
    "    T = defaultdict(list)\n",
    "    D = []\n",
    "\n",
    "    with tqdm(total=len(topics)) as pbar:\n",
    "        for t in reversed(topics):\n",
    "            pbar.update(1)\n",
    "            P[t['Author uid']].append((remove_non_ascii(cleanup(t[\"Body\"])),1))\n",
    "            T[t['Author uid']].append(t['Post date'])\n",
    "            D.append(datetime.strptime(t['Post date'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "    with tqdm(total=len(posts)) as pbar:\n",
    "        for p in reversed(posts):\n",
    "            pbar.update(1)\n",
    "            P[p['Auteur-uid']].append((remove_non_ascii(cleanup(p[\"Body\"])),0))\n",
    "            T[p['Auteur-uid']].append(p['Datum van inzending'])\n",
    "            D.append(datetime.strptime(p['Datum van inzending'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "    return (P,T,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_binlist(D,timetick=1):\n",
    "    lower = min(D)\n",
    "    upper = max(D)\n",
    "\n",
    "    if lower.time()>=datetime.strptime('4:00','%H:%M').time():\n",
    "        lower = lower.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        lower = (lower+timedelta(days = -1)).replace(hour=4,minute=0)\n",
    "\n",
    "    if upper.time()<datetime.strptime('12:00','%H:%M').time():\n",
    "        upper = upper.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        upper = (upper+timedelta(days=1)).replace(hour=4,minute=0)\n",
    "\n",
    "    return([lower + timedelta(hours=x) for x in range(0, 24*(upper-lower).days, timetick)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_questionmarks(body):\n",
    "    Q = 0\n",
    "    for sentence in sent_tokenize(body):\n",
    "        if re.search('\\?+', sentence):\n",
    "            Q+=1\n",
    "    if len(sent_tokenize(body))!=0:\n",
    "        return float(Q)/float(len(sent_tokenize(body)))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average sentiment score per sentence in the string, calculated by pattern\n",
    "def determine_sentiment(body):\n",
    "    S = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        S.append(sentiment(sentence)[0])\n",
    "    return np.mean(S)\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average objectivity score per sentence in the string, calculated by pattern\n",
    "def determine_objectivity(body):\n",
    "    O = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        O.append(sentiment(sentence)[1])\n",
    "    return np.mean(O)\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the total nr of sentences the string is built of\n",
    "def determine_length(body):\n",
    "    # in sentences:\n",
    "    return(len(sent_tokenize(body)))\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average nr of words in sentences that occur in the string\n",
    "def determine_sent_length(body):\n",
    "    # in words:\n",
    "    L = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        L.append(len(word_tokenize(sentence)))\n",
    "    return np.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_variables(pp,user,nr_of_posts,mean_quest,mean_object,mean_sents,mean_length,mean_sents_length,nr_of_starts,nr_of_responses):\n",
    "    fig = plt.figure(1)\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    posts, = ax.plot(nr_of_posts.values(),'b.', label = 'nr of posts', alpha = 0.5) #blue\n",
    "    senlength, = ax.plot(mean_sents_length.values(),'k.', label = 'mean sentence length (words)',alpha = 0.5) #yellow\n",
    "    postlength, = ax.plot(mean_length.values(),'c.', label = 'mean post length (sentences)',alpha = 0.5) #cyan\n",
    "    starts, = ax.plot(nr_of_starts.values(),'g.', label = 'start posts', alpha = 0.5) #black\n",
    "    #responses, = ax.plot(nr_of_responses.values(),'y.', label = 'response posts', alpha = 0.5)\n",
    "    \n",
    "    first_legend = plt.legend(handles=[posts,senlength,postlength,starts], title = \"left axis\", loc='upper left', bbox_to_anchor=(0, -0.1),ncol=1)\n",
    "    axx = plt.gca().add_artist(first_legend)\n",
    "    \n",
    "    ax1 = ax.twinx()\n",
    "    qmarks, = ax1.plot(mean_quest.values(),'y.', label = 'question ratio',alpha = 0.5) #green\n",
    "    ovalues, = ax1.plot(mean_object.values(),'r.', label = 'subjectivity',alpha = 0.5) #red\n",
    "    svalues, = ax1.plot(mean_sent.values(),'m.', label = 'sentiment',alpha = 0.5) #magenta\n",
    "    \n",
    "    \n",
    "    second_legend = plt.legend(handles=[qmarks,ovalues,svalues], title = \"right axis\",loc='upper right', bbox_to_anchor=(1, -0.1),ncol=1)\n",
    "\n",
    "    ax.set_ylim(0,40)\n",
    "    ax1.set_ylim(-1,1)\n",
    "    \n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height, box.width, box.height * 0.7])\n",
    "    box = ax1.get_position()\n",
    "    ax1.set_position([box.x0, box.y0 + box.height, box.width, box.height * 0.7])\n",
    "    \n",
    "    plt.title(user)\n",
    "    plt.show()\n",
    "    pp.savefig(fig, dpi = 300, transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_two(user,x,y):\n",
    "    x = x.values()\n",
    "    y = y.values()\n",
    "    \n",
    "    plt.plot(x,y, 'bo', alpha = 0.1)\n",
    "    plt.xlabel('x-axis variable')\n",
    "    plt.ylabel('y-axis variable')\n",
    "    plt.xlim(0,30)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.title(user)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determine which variables are non-normal, and make a mask for correlations that are measured nonparametrically\n",
    "def mask_nonnormal(df,mask):\n",
    "    for column in df:\n",
    "        index = df.columns.get_loc(column)\n",
    "        z,p = normaltest(df[column].tolist())\n",
    "        # als de data in de kolom significant niet-normaal verdeeld is, activeer dan het maskeer voor de relevante rij en kolom\n",
    "        if p<0.05:\n",
    "            mask[index]=True #rijen blokkeren\n",
    "            mask[0::1,index]=True #kolommen blokkeren\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualise(df, method):\n",
    "    global averagedict\n",
    "    rho = df.corr(method = m)\n",
    "    mask = np.zeros_like(rho, dtype=np.bool)\n",
    "    \n",
    "    if method == 'pearson' or method == 'spearman':\n",
    "        mask = mask_nonnormal(df,mask)\n",
    "        cmap = sbn.diverging_palette(120, 260, n=21, s=80)\n",
    "        cbar_kws= dict(use_gridspec=False,location=\"left\",label= \"Pearson's Rho\")\n",
    "    else:\n",
    "        mask = ~mask_nonnormal(df,mask)\n",
    "        cmap = sbn.diverging_palette(10, 30, n=21, s=99, l=65)\n",
    "        cbar_kws = dict(use_gridspec=False,location=\"right\",label= \"Kendall's Tau\")\n",
    "\n",
    "    #mask the upper half of the figure\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "    # calculate averages        \n",
    "    for i,row in enumerate(mask):\n",
    "        for j,column in enumerate(row):\n",
    "            if column == False: # all unmasked positions\n",
    "                averagedict[i,j].append(rho.iloc[i][j])\n",
    "                if method == 'pearson' or method == 'spearman':\n",
    "                    normaldict[i,j].append(rho.iloc[i][j])\n",
    "                else:\n",
    "                    nonnormaldict[i,j].append(rho.iloc[i][j])\n",
    "                    countnonnormal[i,j].append(1)\n",
    "    \n",
    "    # make heatmap\n",
    "    ax = sbn.heatmap(rho, mask=mask, cbar_kws = cbar_kws, cmap=cmap, vmin = -1, vmax = 1, annot = True, fmt='1.2f')\n",
    "    plt.title('results of (non-)parametric correlation tests')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_correlations(dictionaries,countnonnormal,rho):\n",
    "    for d in dictionaries:\n",
    "        frame = pd.DataFrame().reindex_like(rho)\n",
    "        for (i,j) in d:\n",
    "            frame.iloc[i][j]=np.nanmean(d[(i,j)])\n",
    "        print frame\n",
    "\n",
    "    frame = pd.DataFrame().reindex_like(rho)\n",
    "    for (i,j) in countnonnormal:\n",
    "        frame.iloc[i][j]=np.nansum(countnonnormal[(i,j)])\n",
    "    print frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_week_activity(first_date,last_date,bindict):\n",
    "    for d in range(0, (last_date-first_date).days,7):\n",
    "        week_start = first_date+timedelta(days=d)\n",
    "        week_end = first_date+timedelta(days=d+7)\n",
    "        for date in list(itertools.chain.from_iterable(bindict.values())):\n",
    "            if week_start<=datetime.strptime(date, '%d/%m/%Y - %H:%M')<week_end:\n",
    "                weekcountdict[week_start,week_end].append(1)\n",
    "        if len(weekcountdict[week_start,week_end])==0:\n",
    "            weekcountdict[week_start,week_end] = []\n",
    "    return weekcountdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_past_activity(bindict,index,hours_back=24):\n",
    "    if 0<= index-hours_back<=len(bindict):\n",
    "        past_activity = np.sum([len(bindict[binlist[index-x],binlist[index+1-x]]) for x in range(hours_back)])\n",
    "    else:\n",
    "        past_activity=0\n",
    "    return past_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_information(user, T, first_date,last_date,activity,activity_nonempty,weekcount):\n",
    "    print \"User:\", user\n",
    "    print \"posted one or more posts in\", len(T[user]), \"'bins'.\" \n",
    "    print \"The first post: \", first_date\n",
    "    print \"The last post: \", last_date\n",
    "    print \"Activity spread over: \", last_date-first_date\n",
    "    print \"The average nr of posts per week: \", np.mean(activity), \"including long times of inactivity.\"\n",
    "    print \"The average nr of posts in non-empty weeks: \", np.mean(activity_nonempty)\n",
    "    print \"The range of activity: \", min([len(x) for x in weekcount.values()]), \" to \", max([len(x) for x in weekcount.values()]), \" posts per week\"\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_active_users(include = ['1144']):\n",
    "    global over_treshold\n",
    "    for user in T:\n",
    "        if len(T[user])<30:\n",
    "            pass\n",
    "        elif user not in include:\n",
    "            pass\n",
    "        else:\n",
    "            over_treshold.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99cbd1ee5624c39ba94640d108bb16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47de90314e42470ea7339d2c2822e5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PTD = make_P_T_and_D(topics,posts) \n",
    "P = PTD[0]\n",
    "T = PTD[1]\n",
    "D = PTD[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6562d6d0dd8f477196eb510aa045769f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7e7bf87822407c9e5df865321cd2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\tqdm\\_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _ \n",
      "|_|\n"
     ]
    }
   ],
   "source": [
    "# path for saving csv files\n",
    "path = r\"C:\\Users\\sternheimam\\Desktop\\my-notebook\\user-csvs\" \n",
    "\n",
    "#---------------------------\n",
    "# global variables\n",
    "#---------------------------\n",
    "# chronological list of (lower) bin boundaries (default time tick = 1 hour)\n",
    "binlist = make_binlist(D)\n",
    "\n",
    "# determine the negative difference for measuring the 'backtrack' feature\n",
    "neg_diff = 24\n",
    "\n",
    "\n",
    "#---------------------------\n",
    "# determine 'relevant' users\n",
    "#---------------------------\n",
    "# initiate empty list of users relevant to measure\n",
    "over_treshold = []\n",
    "determine_active_users() # for practical reasons, right now only user 1144 is selected. If more users desired, simply enter them in list form.\n",
    "\n",
    "#---------------------------\n",
    "# go through all users in over_treshold, and do stuff..\n",
    "#---------------------------\n",
    "# show the progress, while going through the active users\n",
    "with tqdm(total=len(over_treshold)) as processbar:\n",
    "    for user in over_treshold:\n",
    "        print user,\n",
    "        processbar.update(1)\n",
    "        \n",
    "        #---------------------------\n",
    "        # initiate some user-specific variables\n",
    "        #---------------------------\n",
    "        first_date = 0\n",
    "        last_date = 0\n",
    "        inactivity = 0\n",
    "        \n",
    "        # all lists starting with csv_ are lists that will eventually contain all values that end up in the csv file\n",
    "        csv_date = []            #datetime\n",
    "        csv_sentiment = []       #sentiment value (-1 to 1)\n",
    "        csv_questionmarks = []   #question mark-ending sentences (float)\n",
    "        csv_subjectivity = []    #subjectivity value (0 to 1)\n",
    "        csv_sentencelength = []  #length of sentence in words (float)\n",
    "        csv_postlength = []      #length of post in sentences (float)\n",
    "        csv_startposts = []      #1 for a thread start, 0 for a response (float) \n",
    "        csv_inactivity = []      #hours passed since last activity (int)\n",
    "        csv_backtrack = []       #posts posted in last x hours (x = neg_diff; default 24h)\n",
    "        \n",
    "        # dictionaries to keep track of activity within certain time bins\n",
    "        bindict = defaultdict(list)        #bins with size 'timetick' (default 1h), values = post-times \n",
    "        postdict = defaultdict(list)       #bins with size 'timetick' (default 1h), values = posts\n",
    "        metadict = defaultdict(list)       #bins with size 'timetick' (default 1h), values = 1 or 0 (start or response)\n",
    "        weekcountdict = defaultdict(list)  #bins with size = 7 days, values = '1' for every post\n",
    "        backtrackdict = defaultdict(list)  #bins with size = neg_diff (default 24h), values = nr of posts in last neg_diff hours\n",
    "        # TO DO: linguistic markers, like adjectives / pronouns / emoticons, and the diversity of topics / vocabulary\n",
    "        \n",
    "        \n",
    "        #---------------------------\n",
    "        # loop through the (sorted) list of datetimes, and do stuff..\n",
    "        #---------------------------\n",
    "        for index,boundary in enumerate(tqdm(binlist)):\n",
    "            # determine time bin boundaries for dictionaries\n",
    "            if index+1>=len(binlist):\n",
    "                break\n",
    "            else:\n",
    "                lower = binlist[index]\n",
    "                upper = binlist[index+1]\n",
    "                \n",
    "                #---------------------------\n",
    "                # Loop through T, collecting all activity for the selected user\n",
    "                #---------------------------\n",
    "                # determine in which time bin the user's activity belongs\n",
    "                for time in T[user]:\n",
    "                    if lower<=datetime.strptime(time, '%d/%m/%Y - %H:%M')<upper:\n",
    "                        bindict[lower,upper].append(time)\n",
    "                        \n",
    "                        # and determine how active the user has been in past neg_diff hours\n",
    "                        past_activity = determine_past_activity(bindict,index,neg_diff)\n",
    "                        backtrackdict[lower,upper].append(past_activity)\n",
    "                        \n",
    "                        # determine the first and last active dates\n",
    "                        if first_date == 0:\n",
    "                            first_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                            last_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                        else:\n",
    "                            last_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                        \n",
    "                        # split the text in P from the start/response-information\n",
    "                        body = P[user][T[user].index(time)][0]\n",
    "                        meta = P[user][T[user].index(time)][1]                        \n",
    "                        postdict[lower,upper].append(body) \n",
    "                        metadict[lower,upper].append(meta)\n",
    "\n",
    "                # fill up the still-empty places in the dictionary\n",
    "                if len(bindict[lower,upper])==0:\n",
    "                    bindict[lower,upper]=[]\n",
    "                    postdict[lower,upper]=[]\n",
    "                    metadict[lower,upper]=[]\n",
    "                    backtrackdict[lower,upper]=[]\n",
    "                \n",
    "                #---------------------------\n",
    "                # Fill csv_feature-lists with values \n",
    "                #---------------------------                \n",
    "                # Treat different posts within same bin 'as one' (concatenate them)\n",
    "                body = '. '.join(postdict[lower,upper]) #can be empty!\n",
    "            \n",
    "                # when then bin is empty, only add 1 to the inactivity feature\n",
    "                if len(body) == 0:\n",
    "                    inactivity+=1\n",
    "                # when the bin is not empty, append a value to all csv_feature-lists (and reset 'inactivity')\n",
    "                else:    \n",
    "                    csv_date.append(lower)\n",
    "                    csv_sentiment.append(determine_sentiment(body))\n",
    "                    csv_questionmarks.append(determine_questionmarks(body))\n",
    "                    csv_subjectivity.append(determine_objectivity(body))\n",
    "                    csv_sentencelength.append(determine_sent_length(body))\n",
    "                    csv_postlength.append(np.mean([determine_length(x) for x in postdict[lower,upper]]))\n",
    "                    csv_startposts.append(np.mean(metadict[lower,upper]))\n",
    "                    csv_inactivity.append(inactivity)\n",
    "                    csv_backtrack.append(np.sum(backtrackdict[lower,upper][-1]))\n",
    "                    inactivity = 0\n",
    "\n",
    "        #---------------------------\n",
    "        # Report the results for this user\n",
    "        #--------------------------- \n",
    "        # determine average activity: over active period, and over only-active weeks \n",
    "        weekcount = determine_week_activity(first_date,last_date,bindict)   \n",
    "        activity = [len(x) for x in weekcount.values()]\n",
    "        activity_nonempty = [len(x) for x in weekcount.values() if not x==[]]\n",
    "        #print_information(user, T, first_date,last_date,activity,activity_nonempty,weekcount)            \n",
    "                \n",
    "        # put all csv_features into a dataframe\n",
    "        df = pd.DataFrame({\"Date & Time\": csv_date, \"Sentiment\": csv_sentiment, \"Questions\": csv_questionmarks, \n",
    "                           \"Subjectivity\": csv_subjectivity, \"Words/Sentence\": csv_sentencelength, \n",
    "                           \"Sentences/Post\": csv_postlength, \"First posts\": csv_startposts,\n",
    "                           \"Inactivity\": csv_inactivity, \"posts in last 24H\": csv_backtrack}).dropna()\n",
    "        name = \"features_user_\"+str(user)+\".csv\"       \n",
    "        df.to_csv(os.path.join(path,name),index=False)\n",
    "print \"_ \"\n",
    "print \"|_|\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
