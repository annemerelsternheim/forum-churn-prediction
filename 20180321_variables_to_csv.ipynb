{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import json, re, time, unicodedata, unidecode, itertools, os\n",
    "from collections import defaultdict\n",
    "from datetime import date, datetime, timedelta\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pattern.nl import sentiment\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# files\n",
    "#MWE\n",
    "#topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE_topic.json'))\n",
    "#posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE.json'))\n",
    "#regular\n",
    "forums = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-35-45 _amazones_forums_export.json'))\n",
    "# zorg voor een nieuwe versie van dit bestand; verkeerd opgeslagen dus je mist het kontje!\n",
    "topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-36-51_amazones_forum_topics_export.json'))\n",
    "posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_forum_posts_export.json'))\n",
    "users = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_users_export.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    \"\"\" this function expects a string, and removes non-ascii characters from it \"\"\"\n",
    "    return unidecode.unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    \"\"\" this function expects a string (post from the BVN/Amazones forum), and returns a cleaner version of it \"\"\"\n",
    "    # remove all links, images, quotes, and emailaddresses\n",
    "    text=re.sub('<a.*?>(.*?)</a>','',text) #remove links\n",
    "    text=re.sub('(http:|www)\\S*','',text) #remove links without markup\n",
    "    text=re.sub('\\[\\\\\\/url\\]','',text)\n",
    "    text=re.sub('<img.*?/>', '',text) #remove images\n",
    "    text=re.sub('<div class=\"bb-quote\">((\\s|\\S)*?)</div>','',text) #remove quotes\n",
    "    text=re.sub('<script.*?>([\\S\\s]*?)</script>','',text) #remove emailaddresses\n",
    "\n",
    "    # replace all emoticon-icons\n",
    "    text=re.sub('<img.*?title=\"(.*?)\".*?/>', '(EMO:\\\\1)',text) #replace emoticons by textual indicators \n",
    "\n",
    "    # replace (most) sideways latin emoticons\n",
    "    text=re.sub('[^>]:-?(\\)|\\])','(EMO:smiley)',text)\n",
    "    text=re.sub(u'☺️','(EMO:smiley)',text)\n",
    "    text=re.sub('[^>]:-?(\\(|\\[)','(EMO:sad)',text)\n",
    "    text=re.sub(';-?(\\)|\\])','(EMO:wink)',text)\n",
    "    text=re.sub(r'(:|;|x|X)-?(D)+\\b','(EMO:laugh)',text)\n",
    "    text=re.sub(':-?(/|\\\\\\|\\|)','(EMO:frown)',text)\n",
    "    text=re.sub(r'(:|;)-?(p|P)+\\b','(EMO:cheeky)',text)\n",
    "    text=re.sub('(:|;)(\\'|\\\")-?(\\(|\\[)','(EMO:cry)',text)\n",
    "    text=re.sub('\\<3+','(EMO:heart)',text)\n",
    "    text=re.sub(u'❤️','(EMO:heart)',text)\n",
    "    text=re.sub('((\\>:-?(\\(|\\]))|(\\>?:-?@))','(EMO:angry)',text)\n",
    "    text=re.sub('\\>:-?(\\)|\\])','(EMO:evil)',text)\n",
    "    text=re.sub(r'(:|;)-?(O|o|0)+\\b','(EMO:shock)',text)\n",
    "    text=re.sub('(:|;)-?(K|k|x|X)','(EMO:kiss)',text)\n",
    "    # :s\n",
    "    # :x is eigenlijk geen kus, geloof ik...\n",
    "\n",
    "    #other important adjustments:\n",
    "    text=re.sub('m\\'?n\\s','mijn ',text) # replacing m'n and mn with mijn, so it gets parsed correctly.\n",
    "    text=re.sub('z\\'?n\\s','zijn ',text) #replacing z'n and zn with zijn\n",
    "    text=re.sub('d\\'?r\\s','haar ',text) #replacing d'r and dr with zijn (only if followed by space, so dr. stays dr.)\n",
    "\n",
    "    # replace all emoticons (and other things) written between double colons\n",
    "    text=re.sub(':([a-zA-Z]+):','(EMO:\\\\1)',text)\n",
    "\n",
    "    # remove remaining markup\n",
    "    text=re.sub('</?(ol|style|b|p|em|u|i|strong|br|span|div|blockquote|li)(.*?)/?>','',text)\n",
    "    text=re.sub('(\\[|\\]|\\{|\\})', '',text)\n",
    "\n",
    "    # separate text from punctuation (may cause double/triple spaces - does not matter at this point)\n",
    "    text = re.sub('(\\.{2,}|/|\\)|,|!|\\?)','\\\\1 ',text) # space behind\n",
    "    text=re.sub('(/|\\()',' \\\\1',text) # space in front\n",
    "    text=re.sub('(\\w{2,})(\\.|,)','\\\\1 \\\\2 ',text) #space 'between'\n",
    "\n",
    "    return(remove_non_ascii(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_P_T_and_D(topics,posts):\n",
    "    \"\"\" this function takes the .json files containing the thread starts and responses, and returns three things:\n",
    "    [0]: a dictionary with the user-ID as key, and the post as value;\n",
    "    [1]: a dictionary with the user-ID as key, and the time of posting as a value;\n",
    "    [2]: a list of all datetimes present in the data (sorted by date, because the .json was already sorted) \"\"\"\n",
    "    P = defaultdict(list)\n",
    "    T = defaultdict(list)\n",
    "    D = []\n",
    "\n",
    "    with tqdm(total=len(topics)) as pbar:\n",
    "        for t in reversed(topics):\n",
    "            pbar.update(1)\n",
    "            P[t['Author uid']].append((cleanup(t[\"Body\"]),1))\n",
    "            T[t['Author uid']].append(t['Post date'])\n",
    "            D.append(datetime.strptime(t['Post date'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "    with tqdm(total=len(posts)) as pbar:\n",
    "        for p in reversed(posts):\n",
    "            pbar.update(1)\n",
    "            P[p['Auteur-uid']].append((cleanup(p[\"Body\"]),0))\n",
    "            T[p['Auteur-uid']].append(p['Datum van inzending'])\n",
    "            D.append(datetime.strptime(p['Datum van inzending'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "    return (P,T,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2']\n"
     ]
    }
   ],
   "source": [
    "over_treshold = []\n",
    "T = {'1': [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "     '2': [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}\n",
    "determine_active_users(['2'])\n",
    "print over_treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_active_users(include_only = []):\n",
    "    global over_treshold\n",
    "    \n",
    "    if include_only == []:\n",
    "        userlist = T\n",
    "    else:\n",
    "        userlist = include_only\n",
    "    for user in userlist:\n",
    "        if len(T[user])<30:\n",
    "            pass\n",
    "        else:\n",
    "            over_treshold.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_binlist(D,timetick=1):\n",
    "    \"\"\" this function takes a list of dates (D), and generates a new list of dates,\n",
    "    starting at 4:00 AM just before the earliest date in D, and ending at 4:00 just after the latest date in D,\n",
    "    with fixed timeticks between all dates in the list.\n",
    "    Optionally, the length of the timetick may be specified (in hours).\n",
    "    \"\"\"\n",
    "    lower = min(D)\n",
    "    upper = max(D)\n",
    "\n",
    "    if lower.time()>=datetime.strptime('4:00','%H:%M').time():\n",
    "        lower = lower.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        lower = (lower+timedelta(days = -1)).replace(hour=4,minute=0)\n",
    "\n",
    "    if upper.time()<datetime.strptime('12:00','%H:%M').time():\n",
    "        upper = upper.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        upper = (upper+timedelta(days=1)).replace(hour=4,minute=0)\n",
    "\n",
    "    return([lower + timedelta(hours=x) for x in range(0, 24*(upper-lower).days, timetick)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_questionmarks(body, Q=0):\n",
    "    \"\"\" This function counts and returns the number of sentences in the provided input string\n",
    "    that ends in at least one question mark \"\"\"\n",
    "    for sentence in sent_tokenize(body):\n",
    "        if re.search('\\?+', sentence):\n",
    "            Q+=1\n",
    "    if len(sent_tokenize(body))!=0:\n",
    "        return float(Q)/float(len(sent_tokenize(body)))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def determine_sentiment(body):\n",
    "    \"\"\" this funciton determines and returns the average sentiment of sentences in the provided input string.\n",
    "    It uses the pattern module to do so. Sentiment values may range from -1 to 1. \"\"\"\n",
    "    return np.mean([sentiment(sentence)[0] for sentence in sent_tokenize(body)]) \n",
    "\n",
    "def determine_subjectivity(body):\n",
    "    \"\"\" This function determines and returns the average subjectivity of sentences in the provided input string.\n",
    "    It uses the pattern module to do so. Subjectivity values may range from 0 to 1. \"\"\"\n",
    "    return np.mean([sentiment(sentence)[1] for sentence in sent_tokenize(body)]) \n",
    "\n",
    "def determine_post_length(body):\n",
    "    \"\"\" This function determines and returns the length of the provided input string in sentences.\n",
    "    It uses the nltk sent_tokenize function to do so. \"\"\"\n",
    "    return(len(sent_tokenize(body)))\n",
    "\n",
    "def determine_sentence_length(body):\n",
    "    \"\"\" This function determines and returns the average length of the sentences in the provided input string in words.\n",
    "    It uses the nltk word_tokenize function to do so. \"\"\"\n",
    "    #word_tokenize also considers interpunction a word\n",
    "    return np.mean([len(word_tokenize(sentence)) for sentence in sent_tokenize(body)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_week_activity(first_date,last_date,bindict):\n",
    "    \"\"\" This function returns a dictionary that has kept track of the activity in week-bins, instead of day-bins.\n",
    "    It expects two dates, to indicate in between which dates the dictionary should be built,\n",
    "    and expects a dictionary in which all the user's active times are already stored\"\"\"\n",
    "    for d in range(0, (last_date-first_date).days,7):\n",
    "        week_start = first_date+timedelta(days=d)\n",
    "        week_end = first_date+timedelta(days=d+7)\n",
    "        for date in list(itertools.chain.from_iterable(bindict.values())):\n",
    "            if week_start<=datetime.strptime(date, '%d/%m/%Y - %H:%M')<week_end:\n",
    "                weekcountdict[week_start,week_end].append(1)\n",
    "        if len(weekcountdict[week_start,week_end])==0:\n",
    "            weekcountdict[week_start,week_end] = []\n",
    "    return weekcountdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_past_activity(bindict,index,hours_back=24):\n",
    "    \"\"\" This function returns the number of times a user has been active in the last hours_back hours (default 24h).\n",
    "    The first hours_back time bins will for now have a value of 0 by default, to keep things easy.\"\"\"\n",
    "    if 0<= index-hours_back<=len(bindict):\n",
    "        past_activity = np.sum([len(bindict[binlist[index-(x+1)],binlist[index+1-(x+1)]]) for x in range(hours_back)])\n",
    "    else:\n",
    "        past_activity=0\n",
    "    return past_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_information(user, T, first_date,last_date,weekcount):\n",
    "    \"\"\" this function prints useful basic information on the user's activity.\n",
    "    It needs quite some input so make sure you've got them all:\n",
    "    1) user ID, 2) a dictionary containing users-IDs as key, and any activity log as value,\n",
    "    3,4) the first and last date of activity, and 5) the dictionary that kept track of the week activity. \"\"\"\n",
    "    print \"User:\", user\n",
    "    print \"posted one or more posts in\", len(T[user]), \"'bins'.\" \n",
    "    print \"The first post: \", first_date\n",
    "    print \"The last post: \", last_date\n",
    "    print \"Activity spread over: \", last_date-first_date\n",
    "    print \"The average nr of posts per week: \", np.mean([len(x) for x in weekcount.values()]), \"including long times of inactivity.\"\n",
    "    print \"The average nr of posts in non-empty weeks: \", np.mean([len(x) for x in weekcount.values() if not x==[]])\n",
    "    print \"The range of activity: \", min([len(x) for x in weekcount.values()]), \" to \", max([len(x) for x in weekcount.values()]), \" posts per week\"\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a3659b5c68436cb34e73f968b47da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1e09f494fb47819a44952151d45e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PTD = make_P_T_and_D(topics,posts) \n",
    "P = PTD[0]\n",
    "T = PTD[1]\n",
    "D = PTD[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c686dabb90499a9493d82cc5f0b219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0b4359b7f14b4fa59360545b13821c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " User: 1144\n",
      "posted one or more posts in 30 'bins'.\n",
      "The first post:  2005-06-21 11:44:00\n",
      "The last post:  2009-11-30 22:24:00\n",
      "Activity spread over:  1623 days, 10:40:00\n",
      "The average nr of posts per week:  0.12931034482758622 including long times of inactivity.\n",
      "The average nr of posts in non-empty weeks:  1.3636363636363635\n",
      "The range of activity:  0  to  3  posts per week\n",
      "\n",
      "\n",
      " _ \n",
      "|_|\n"
     ]
    }
   ],
   "source": [
    "# path for saving csv files\n",
    "path = r\"C:\\Users\\sternheimam\\Desktop\\my-notebook\\user-csvs\" \n",
    "\n",
    "#---------------------------\n",
    "# global variables\n",
    "#---------------------------\n",
    "# chronological list of (lower) bin boundaries (default time tick = 1 hour)\n",
    "binlist = make_binlist(D)\n",
    "\n",
    "# determine the negative difference for measuring the 'backtrack' feature\n",
    "neg_diff = 24\n",
    "\n",
    "\n",
    "#---------------------------\n",
    "# determine 'relevant' users\n",
    "#---------------------------\n",
    "# initiate empty list of users relevant to measure\n",
    "over_treshold = []\n",
    "determine_active_users(['1144']) # for practical reasons, right now only user 1144 is selected. If more users desired, simply enter them in list form.\n",
    "\n",
    "#---------------------------\n",
    "# go through all users in over_treshold, and do stuff..\n",
    "#---------------------------\n",
    "# show the progress, while going through the active users\n",
    "with tqdm(total=len(over_treshold)) as processbar:\n",
    "    for user in over_treshold:\n",
    "        print user,\n",
    "        processbar.update(1)\n",
    "        \n",
    "        #---------------------------\n",
    "        # initiate some user-specific variables\n",
    "        #---------------------------\n",
    "        first_date = 0\n",
    "        last_date = 0\n",
    "        inactivity = 0\n",
    "        \n",
    "        # all lists starting with csv_ are lists that will eventually contain all values that end up in the csv file\n",
    "        csv_date = []            #datetime\n",
    "        csv_sentiment = []       #sentiment value (-1 to 1)\n",
    "        csv_questionmarks = []   #question mark-ending sentences (float)\n",
    "        csv_subjectivity = []    #subjectivity value (0 to 1)\n",
    "        csv_sentencelength = []  #length of sentence in words (float)\n",
    "        csv_postlength = []      #length of post in sentences (float)\n",
    "        csv_startposts = []      #1 for a thread start, 0 for a response (float) \n",
    "        csv_inactivity = []      #hours passed since last activity (int)\n",
    "        csv_backtrack = []       #posts posted in last x hours (x = neg_diff; default 24h)\n",
    "        \n",
    "        # dictionaries to keep track of activity within certain time bins\n",
    "        bindict = defaultdict(list)        #bins with size 'timetick' (default 1h), values = post-times \n",
    "        postdict = defaultdict(list)       #bins with size 'timetick' (default 1h), values = posts\n",
    "        metadict = defaultdict(list)       #bins with size 'timetick' (default 1h), values = 1 or 0 (start or response)\n",
    "        weekcountdict = defaultdict(list)  #bins with size = 7 days, values = '1' for every post\n",
    "        backtrackdict = defaultdict(list)  #bins with size = neg_diff (default 24h), values = nr of posts in last neg_diff hours\n",
    "        # TO DO: linguistic markers, like adjectives / pronouns / emoticons, and the diversity of topics / vocabulary\n",
    "        \n",
    "        \n",
    "        #---------------------------\n",
    "        # loop through the (sorted) list of datetimes, and do stuff..\n",
    "        #---------------------------\n",
    "        for index,boundary in enumerate(tqdm(binlist)):\n",
    "            # determine time bin boundaries for dictionaries\n",
    "            if index+1>=len(binlist):\n",
    "                break\n",
    "            else:\n",
    "                lower = binlist[index]\n",
    "                upper = binlist[index+1]\n",
    "                \n",
    "                #---------------------------\n",
    "                # Loop through T, collecting all activity for the selected user\n",
    "                #---------------------------\n",
    "                # determine in which time bin the user's activity belongs\n",
    "                for time in T[user]:\n",
    "                    if lower<=datetime.strptime(time, '%d/%m/%Y - %H:%M')<upper:\n",
    "                        bindict[lower,upper].append(time)\n",
    "                        \n",
    "                        # and determine how active the user has been in past neg_diff hours\n",
    "                        past_activity = determine_past_activity(bindict,index,neg_diff)\n",
    "                        backtrackdict[lower,upper].append(past_activity)\n",
    "                        \n",
    "                        # determine the first and last active dates\n",
    "                        if first_date == 0:\n",
    "                            first_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                            last_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                        else:\n",
    "                            last_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                        \n",
    "                        # split the text in P from the start/response-information\n",
    "                        body = P[user][T[user].index(time)][0]\n",
    "                        meta = P[user][T[user].index(time)][1]                        \n",
    "                        postdict[lower,upper].append(body) \n",
    "                        metadict[lower,upper].append(meta)\n",
    "\n",
    "                # fill up the still-empty places in the dictionary\n",
    "                if len(bindict[lower,upper])==0:\n",
    "                    bindict[lower,upper]=[]\n",
    "                    postdict[lower,upper]=[]\n",
    "                    metadict[lower,upper]=[]\n",
    "                    backtrackdict[lower,upper]=[]\n",
    "                \n",
    "                #---------------------------\n",
    "                # Fill csv_feature-lists with values \n",
    "                #---------------------------                \n",
    "                # Treat different posts within same bin 'as one' (concatenate them)\n",
    "                body = '. '.join(postdict[lower,upper]) #can be empty!\n",
    "            \n",
    "                # when then bin is empty, only add 1 to the inactivity feature\n",
    "                if len(body) == 0:\n",
    "                    inactivity+=1\n",
    "                # when the bin is not empty, append a value to all csv_feature-lists (and reset 'inactivity')\n",
    "                else:    \n",
    "                    csv_date.append(lower)\n",
    "                    csv_sentiment.append(determine_sentiment(body))\n",
    "                    csv_questionmarks.append(determine_questionmarks(body))\n",
    "                    csv_subjectivity.append(determine_subjectivity(body))\n",
    "                    csv_sentencelength.append(determine_sentence_length(body))\n",
    "                    csv_postlength.append(np.mean([determine_post_length(x) for x in postdict[lower,upper]]))\n",
    "                    csv_startposts.append(np.mean(metadict[lower,upper]))\n",
    "                    csv_inactivity.append(inactivity)\n",
    "                    csv_backtrack.append(np.sum(backtrackdict[lower,upper][-1]))\n",
    "                    inactivity = 0\n",
    "\n",
    "        #---------------------------\n",
    "        # Report the results for this user\n",
    "        #--------------------------- \n",
    "        # determine average activity: over active period, and over only-active weeks \n",
    "        weekcount = determine_week_activity(first_date,last_date,bindict)   \n",
    "        print_information(user, T, first_date,last_date,weekcount)            \n",
    "                \n",
    "        # put all csv_features into a dataframe\n",
    "        df = pd.DataFrame({\"Date & Time\": csv_date, \"Sentiment\": csv_sentiment, \"Questions\": csv_questionmarks, \n",
    "                           \"Subjectivity\": csv_subjectivity, \"Words/Sentence\": csv_sentencelength, \n",
    "                           \"Sentences/Post\": csv_postlength, \"First posts\": csv_startposts,\n",
    "                           \"Inactivity\": csv_inactivity, \"Posts in last 24H\": csv_backtrack}).dropna()\n",
    "        name = \"features_user_\"+str(user)+\".csv\"       \n",
    "        df.to_csv(os.path.join(path,name),index=False)\n",
    "print \" _ \"\n",
    "print \"|_|\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
