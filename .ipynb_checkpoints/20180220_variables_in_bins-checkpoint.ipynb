{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import seaborn as sbn\n",
    "import json, re, time, unicodedata, unidecode, codecs, random, math, warnings, itertools\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "from pattern.nl import parse, split, parsetree\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import date, datetime, timedelta\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pattern.nl import sentiment\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import normaltest\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# files\n",
    "#MWE\n",
    "#topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE_topic.json'))\n",
    "#posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE.json'))\n",
    "#regular\n",
    "forums = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-35-45 _amazones_forums_export.json'))\n",
    "# zorg voor een nieuwe versie van dit bestand; verkeerd opgeslagen dus je mist het kontje!\n",
    "topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-36-51_amazones_forum_topics_export.json'))\n",
    "posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_forum_posts_export.json'))\n",
    "users = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_users_export.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove non-ascii characters from file (otherwise they will become part of the tokens)\n",
    "def remove_non_ascii(text):\n",
    "    return unidecode.unidecode(text)\n",
    "    #return ''.join([i if ord(i) < 128 else ' ' for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    #remove all links, images, quotes, and emailaddresses\n",
    "    text=re.sub('<a.*?>(.*?)</a>','',text) #remove links\n",
    "    text=re.sub('(http:|www)\\S*','',text) #remove links without markup\n",
    "    text=re.sub('\\[\\\\\\/url\\]','',text)\n",
    "    text=re.sub('<img.*?/>', '',text) #remove images\n",
    "    text=re.sub('<div class=\"bb-quote\">((\\s|\\S)*?)</div>','',text) #remove quotes\n",
    "    text=re.sub('<script.*?>([\\S\\s]*?)</script>','',text) #remove emailaddresses\n",
    "\n",
    "    #replace all emoticon-icons\n",
    "    text=re.sub('<img.*?title=\"(.*?)\".*?/>', '(EMO:\\\\1)',text) #replace emoticons by textual indicators \n",
    "\n",
    "    # replace (most) sideways latin emoticons\n",
    "    text=re.sub('[^>]:-?(\\)|\\])','(EMO:smiley)',text)\n",
    "    text=re.sub(u'☺️','(EMO:smiley)',text)\n",
    "    text=re.sub('[^>]:-?(\\(|\\[)','(EMO:sad)',text)\n",
    "    text=re.sub(';-?(\\)|\\])','(EMO:wink)',text)\n",
    "    text=re.sub(r'(:|;|x|X)-?(D)+\\b','(EMO:laugh)',text)\n",
    "    text=re.sub(':-?(/|\\\\\\|\\|)','(EMO:frown)',text)\n",
    "    text=re.sub(r'(:|;)-?(p|P)+\\b','(EMO:cheeky)',text)\n",
    "    text=re.sub('(:|;)(\\'|\\\")-?(\\(|\\[)','(EMO:cry)',text)\n",
    "    text=re.sub('\\<3+','(EMO:heart)',text)\n",
    "    text=re.sub(u'❤️','(EMO:heart)',text)\n",
    "    text=re.sub('((\\>:-?(\\(|\\]))|(\\>?:-?@))','(EMO:angry)',text)\n",
    "    text=re.sub('\\>:-?(\\)|\\])','(EMO:evil)',text)\n",
    "    text=re.sub(r'(:|;)-?(O|o|0)+\\b','(EMO:shock)',text)\n",
    "    text=re.sub('(:|;)-?(K|k|x|X)','(EMO:kiss)',text)\n",
    "    # :s\n",
    "    # :x is eigenlijk geen kus, geloof ik...\n",
    "\n",
    "\n",
    "    #other important adjustments:\n",
    "    text=re.sub('m\\'?n\\s','mijn ',text) # replacing m'n and mn with mijn, so it gets parsed correctly.\n",
    "    text=re.sub('z\\'?n\\s','zijn ',text) #replacing z'n and zn with zijn\n",
    "    text=re.sub('d\\'?r\\s','haar ',text) #replacing d'r and dr with zijn (only if followed by space, so dr. stays dr.)\n",
    "\n",
    "    # replace all emoticons (and other things) written between double colons\n",
    "    text=re.sub(':([a-zA-Z]+):','(EMO:\\\\1)',text)\n",
    "\n",
    "    # remove remaining markup\n",
    "    text=re.sub('</?(ol|style|b|p|em|u|i|strong|br|span|div|blockquote|li)(.*?)/?>','',text)\n",
    "    text=re.sub('(\\[|\\]|\\{|\\})', '',text)\n",
    "\n",
    "    # separate text from punctuation (may cause double/triple spaces - does not matter at this point)\n",
    "    text = re.sub('(\\.{2,}|/|\\)|,|!|\\?)','\\\\1 ',text) # space behind\n",
    "    text=re.sub('(/|\\()',' \\\\1',text) # space in front\n",
    "    text=re.sub('(\\w{2,})(\\.|,)','\\\\1 \\\\2 ',text) #space 'between'\n",
    "\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make two dictionaries: user's post T imes, and user's P ost texts.\n",
    "def make_P_T_and_D(topics,posts,count=2500):\n",
    "    P = defaultdict(list)\n",
    "    T = defaultdict(list)\n",
    "    D = []\n",
    "\n",
    "    with tqdm(total=len(topics)) as pbar:\n",
    "        for t in reversed(topics): \n",
    "            pbar.update(1)\n",
    "            P[t['Author uid']].append((remove_non_ascii(cleanup(t[\"Body\"])),1))\n",
    "            T[t['Author uid']].append(t['Post date'])\n",
    "            D.append(datetime.strptime(t['Post date'], '%d/%m/%Y - %H:%M'))\n",
    "    \n",
    "    with tqdm(total=len(posts)) as pbar:\n",
    "        for p in reversed(posts):\n",
    "            pbar.update(1)\n",
    "            P[p['Auteur-uid']].append((remove_non_ascii(cleanup(p[\"Body\"])),0))\n",
    "            T[p['Auteur-uid']].append(p['Datum van inzending'])\n",
    "            D.append(datetime.strptime(p['Datum van inzending'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "    return (P,T,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_binlist(D,timetick=1): #timetick in days\n",
    "    lower = min(D)\n",
    "    upper = max(D)\n",
    "\n",
    "    if lower.time()>=datetime.strptime('4:00','%H:%M').time():\n",
    "        lower = lower.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        lower = (lower+timedelta(days=-1)).replace(hour=4,minute=0)\n",
    "\n",
    "    if upper.time()<datetime.strptime('12:00','%H:%M').time():\n",
    "        upper = upper.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        upper = (upper+timedelta(days=1)).replace(hour=4,minute=0)\n",
    "\n",
    "    return([lower + timedelta(days=x) for x in range(0, (upper-lower).days, timetick)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_questionmarks(body):\n",
    "    Q = 0\n",
    "    for sentence in sent_tokenize(body):\n",
    "        if re.search('\\?+', sentence):\n",
    "            Q+=1\n",
    "    if len(sent_tokenize(body))!=0:\n",
    "        return float(Q)/float(len(sent_tokenize(body)))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average sentiment score per sentence in the string, calculated by pattern\n",
    "def determine_sentiment(body):\n",
    "    S = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        S.append(sentiment(sentence)[0])\n",
    "    return np.mean(S)\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average objectivity score per sentence in the string, calculated by pattern\n",
    "def determine_objectivity(body):\n",
    "    O = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        O.append(sentiment(sentence)[1])\n",
    "    return np.mean(O)\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the total nr of sentences the string is built of\n",
    "def determine_length(body):\n",
    "    # in sentences:\n",
    "    return(len(sent_tokenize(body)))\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average nr of words in sentences that occur in the string\n",
    "def determine_sent_length(body):\n",
    "    # in words:\n",
    "    L = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        L.append(len(word_tokenize(sentence)))\n",
    "    return np.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_variables(pp,user,nr_of_posts,mean_quest,mean_object,mean_sents,mean_length,mean_sents_length,nr_of_starts,nr_of_responses):\n",
    "    fig = plt.figure(1)\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    posts, = ax.plot(nr_of_posts.values(),'b.', label = 'nr of posts', alpha = 1) #blue\n",
    "    senlength, = ax.plot(mean_sents_length.values(),'k.', label = 'mean sentence length (words)',alpha = 0) #yellow\n",
    "    postlength, = ax.plot(mean_length.values(),'c.', label = 'mean post length (sentences)',alpha = 0) #cyan\n",
    "    starts, = ax.plot(nr_of_starts.values(),'g.', label = 'start posts', alpha = 0) #black\n",
    "    #responses, = ax.plot(nr_of_responses.values(),'y.', label = 'response posts', alpha = 0.5)\n",
    "    \n",
    "    first_legend = plt.legend(handles=[posts,senlength,postlength,starts], title = \"left axis\", loc='upper left', bbox_to_anchor=(0, -0.1),ncol=1)\n",
    "    axx = plt.gca().add_artist(first_legend)\n",
    "    \n",
    "    ax1 = ax.twinx()\n",
    "    qmarks, = ax1.plot(mean_quest.values(),'y.', label = 'question ratio',alpha = 0) #green\n",
    "    ovalues, = ax1.plot(mean_object.values(),'r.', label = 'subjectivity',alpha = 0) #red\n",
    "    svalues, = ax1.plot(mean_sent.values(),'m.', label = 'sentiment',alpha = 0) #magenta\n",
    "    \n",
    "    \n",
    "    second_legend = plt.legend(handles=[qmarks,ovalues,svalues], title = \"right axis\",loc='upper right', bbox_to_anchor=(1, -0.1),ncol=1)\n",
    "\n",
    "    ax.set_ylim(0,10)\n",
    "    ax1.set_ylim(-1,1)\n",
    "    \n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height, box.width, box.height * 0.7])\n",
    "    box = ax1.get_position()\n",
    "    ax1.set_position([box.x0, box.y0 + box.height, box.width, box.height * 0.7])\n",
    "    \n",
    "    plt.title(user)\n",
    "    plt.show()\n",
    "    pp.savefig(fig, dpi = 300, transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_two(user,x,y):\n",
    "    x = x.values()\n",
    "    y = y.values()\n",
    "    \n",
    "    plt.plot(x,y, 'bo', alpha = 0.1)\n",
    "    plt.xlabel('questions per post')\n",
    "    plt.ylabel('nr of responses')\n",
    "    plt.xlim(0,10)\n",
    "    plt.ylim(0,10)\n",
    "    plt.title(\"correlation plot\")\n",
    "    #plt.show() # will show for all users separately if enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determine which variables are non-normal, and make a mask for correlations that are measured nonparametrically\n",
    "def mask_nonnormal(df,mask):\n",
    "    for column in df:\n",
    "        index = df.columns.get_loc(column)\n",
    "        z,p = normaltest(df[column].tolist())\n",
    "        # als de data in de kolom significant niet-normaal verdeeld is, activeer dan het maskeer voor de relevante rij en kolom\n",
    "        if p<0.05:\n",
    "            mask[index]=True #rijen blokkeren\n",
    "            mask[0::1,index]=True #kolommen blokkeren\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualise(df, method):\n",
    "    global averagedict, normaldict, nonnormaldict, countnonnormal\n",
    "    rho = df.corr(method = m)\n",
    "    mask = np.zeros_like(rho, dtype=np.bool)\n",
    "    \n",
    "    if method == 'pearson' or method == 'spearman':\n",
    "        mask = mask_nonnormal(df,mask)\n",
    "        cmap = sbn.diverging_palette(120, 260, n=21, s=80)\n",
    "        cbar_kws= dict(use_gridspec=False,location=\"left\",label= \"Pearson's Rho\")\n",
    "    else:\n",
    "        mask = ~mask_nonnormal(df,mask)\n",
    "        cmap = sbn.diverging_palette(10, 30, n=21, s=99, l=65)\n",
    "        cbar_kws = dict(use_gridspec=False,location=\"right\",label= \"Kendall's Tau\")\n",
    "\n",
    "    #mask the upper half of the figure\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "    # calculate averages        \n",
    "    for i,row in enumerate(mask):\n",
    "        for j,column in enumerate(row):\n",
    "            if column == False: # all unmasked positions\n",
    "                averagedict[i,j].append(rho.iloc[i][j])\n",
    "                if method == 'pearson' or method == 'spearman':\n",
    "                    normaldict[i,j].append(rho.iloc[i][j])\n",
    "                else:\n",
    "                    nonnormaldict[i,j].append(rho.iloc[i][j])\n",
    "                    countnonnormal[i,j].append(1)\n",
    "    \n",
    "    # make heatmap\n",
    "    ax = sbn.heatmap(rho, mask=mask, cbar_kws = cbar_kws, cmap=cmap, vmin = -1, vmax = 1, annot = True, fmt='1.2f')\n",
    "    plt.title('results of (non-)parametric correlation tests')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_correlations(dictionaries,countnonnormal,rho):\n",
    "    for d in dictionaries:\n",
    "        frame = pd.DataFrame().reindex_like(rho)\n",
    "        for (i,j) in d:\n",
    "            frame.iloc[i][j]=np.nanmean(d[(i,j)])\n",
    "        print frame\n",
    "\n",
    "    frame = pd.DataFrame().reindex_like(rho)\n",
    "    for (i,j) in countnonnormal:\n",
    "        frame.iloc[i][j]=np.nansum(countnonnormal[(i,j)])\n",
    "    print frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_week_activity(first_date,last_date,bindict):\n",
    "    for d in range(0, (last_date-first_date).days,7):\n",
    "        week_start = first_date+timedelta(days=d)\n",
    "        week_end = first_date+timedelta(days=d+7)\n",
    "        for date in list(itertools.chain.from_iterable(bindict.values())):\n",
    "            if week_start<=datetime.strptime(date, '%d/%m/%Y - %H:%M')<week_end:\n",
    "                weekcountdict[week_start,week_end].append(1)\n",
    "        if len(weekcountdict[week_start,week_end])==0:\n",
    "            weekcountdict[week_start,week_end] = []\n",
    "    return weekcountdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_past_activity(bindict,index,hours_back=24):\n",
    "    if 0<= index-hours_back<=len(bindict):\n",
    "        past_activity = np.sum([len(bindict[binlist[index-x],binlist[index+1-x]]) for x in range(hours_back)])\n",
    "    else:\n",
    "        past_activity=0\n",
    "    return past_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_information(user, T, first_date,last_date,activity,activity_nonempty,weekcount):\n",
    "    print \"User:\", user\n",
    "    print \"posted one or more posts in\", len(T[user]), \"'bins'.\" \n",
    "    print \"The first post: \", first_date\n",
    "    print \"The last post: \", last_date\n",
    "    print \"Activity spread over: \", last_date-first_date\n",
    "    print \"The average nr of posts per week: \", np.mean(activity), \"including long times of inactivity.\"\n",
    "    print \"The average nr of posts in non-empty weeks: \", np.mean(activity_nonempty)\n",
    "    print \"The range of activity: \", min([len(x) for x in weekcount.values()]), \" to \", max([len(x) for x in weekcount.values()]), \" posts per week\"\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb146cc98d2643848ea735ab4ed6858a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25f0070c4a047a48fb6415dfda28d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\tqdm\\_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PTD = make_P_T_and_D(topics,posts) \n",
    "P = PTD[0]\n",
    "T = PTD[1]\n",
    "D = PTD[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f41970e48a8498ebe8ede37911154e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 1144\n",
      "posted one or more posts in 30 'bins'.\n",
      "The first post:  2005-06-21 11:44:00\n",
      "The last post:  2009-11-30 22:24:00\n",
      "Activity spread over:  1623 days, 10:40:00\n",
      "The average nr of posts per week:  0.12931034482758622 including long times of inactivity.\n",
      "The average nr of posts in non-empty weeks:  1.3636363636363635\n",
      "The range of activity:  0  to  3  posts per week\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plt.close()\n",
    "binlist = make_binlist(D,1) #timetick in (whole) days\n",
    "pp = PdfPages(\"plots-author-BVN.pdf\")\n",
    "normaldict = defaultdict(list)\n",
    "nonnormaldict = defaultdict(list)\n",
    "averagedict = defaultdict(list)\n",
    "countnonnormal = defaultdict(list)\n",
    "\n",
    "for user in tqdm(T):\n",
    "    if len(T[user])<30:\n",
    "        pass # go to next user. This one has too little activity\n",
    "    if user != '1144':\n",
    "        pass\n",
    "    else:\n",
    "        first_date = 0\n",
    "        last_date = 0\n",
    "        print \"User:\", user\n",
    "        print \"posted one or more posts in\", len(T[user]), \"'bins'.\"    \n",
    "        \n",
    "        # variables\n",
    "        bindict = defaultdict(list)\n",
    "        postdict = defaultdict(list)\n",
    "        sendict = defaultdict(list)\n",
    "        questdict = defaultdict(list)\n",
    "        objecdict = defaultdict(list)\n",
    "        lengthdict = defaultdict(list)\n",
    "        senlengthdict = defaultdict(list)\n",
    "        startsdict = defaultdict(list)\n",
    "        responsesdict = defaultdict(list)\n",
    "        \n",
    "        weekcountdict = defaultdict(list)\n",
    "        # length of posts (in words, or sentences)\n",
    "        # nr of replies to posts vs nr of starting posts\n",
    "        # linguistic markers, like adjectives / pronouns / emoticons, and the diversity of topics / vocabulary\n",
    "\n",
    "        # variables (plottable)\n",
    "        nr_of_posts = dict()\n",
    "        mean_quest = dict()\n",
    "        mean_sent = dict()\n",
    "        mean_object = dict()\n",
    "        mean_length = dict()\n",
    "        mean_sents_length = dict()\n",
    "        nr_of_starts = dict()\n",
    "        nr_of_responses = dict()\n",
    "\n",
    "        for index,boundary in enumerate(binlist):\n",
    "            if index+1>=len(binlist):\n",
    "                break\n",
    "            else:\n",
    "                lower = binlist[index]\n",
    "                upper = binlist[index+1]\n",
    "\n",
    "                for time in T[user]:\n",
    "                    if lower<=datetime.strptime(time, '%d/%m/%Y - %H:%M')<upper:\n",
    "                        if first_date == 0:\n",
    "                            first_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                            last_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                        else:\n",
    "                            last_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                            \n",
    "                        \n",
    "                        body = P[user][T[user].index(time)][0]\n",
    "                        meta = P[user][T[user].index(time)][1]\n",
    "\n",
    "                        senti = determine_sentiment(body) # average sentiment per sentence in body\n",
    "                        questionmarks = determine_questionmarks(body) # ratio of sentences in body ending with question marks\n",
    "                        objectivity = determine_objectivity(body)\n",
    "                        length = determine_length(body)\n",
    "                        sentence_length = determine_sent_length(body)\n",
    "                        if meta == 0: # 0 is for the response to a thread, 1 is the start of thread\n",
    "                            response = 1\n",
    "                            start = 0\n",
    "                        elif meta == 1:\n",
    "                            response = 0\n",
    "                            start = 1\n",
    "\n",
    "                        bindict[lower,upper].append(time)\n",
    "                        postdict[lower,upper].append(body) \n",
    "                        sendict[lower,upper].append(senti)\n",
    "                        questdict[lower,upper].append(questionmarks)\n",
    "                        objecdict[lower,upper].append(objectivity)\n",
    "                        lengthdict[lower,upper].append(length)\n",
    "                        senlengthdict[lower,upper].append(sentence_length)\n",
    "                        startsdict[lower,upper].append(start)\n",
    "                        responsesdict[lower,upper].append(response)\n",
    "\n",
    "            # fill up empty places in dictionary\n",
    "            if len(bindict[lower,upper])==0:\n",
    "                bindict[lower,upper]=[]\n",
    "                postdict[lower,upper]=[]\n",
    "                sendict[lower,upper]=[]\n",
    "                questdict[lower,upper]=[]\n",
    "                objecdict[lower,upper]=[]\n",
    "                lengthdict[lower,upper]=[]\n",
    "                senlengthdict[lower,upper]=[]\n",
    "                startsdict[lower,upper]=[]\n",
    "                responsesdict[lower,upper]=[]\n",
    "\n",
    "        #convert dictionaries to things you want plotted, like averages:\n",
    "        for lower,upper in bindict: \n",
    "            # mean nr of sentences per timetick\n",
    "            if len(bindict[lower,upper])==0:\n",
    "                nr_of_posts[lower,upper]=float('nan')\n",
    "                mean_quest[lower,upper]=float('nan')\n",
    "                mean_sent[lower,upper]=float('nan')\n",
    "                mean_object[lower,upper]=float('nan')\n",
    "                mean_length[lower,upper]=float('nan')\n",
    "                mean_sents_length[lower,upper]=float('nan')\n",
    "                nr_of_starts[lower,upper]=float('nan')\n",
    "                nr_of_responses[lower,upper]=float('nan')\n",
    "            else:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                    nr_of_posts[lower,upper]=len(bindict[lower,upper])\n",
    "                    mean_quest[lower,upper]=np.mean(questdict[lower,upper])\n",
    "                    mean_sent[lower,upper]=np.mean(sendict[lower,upper])\n",
    "                    mean_object[lower,upper]=np.mean(objecdict[lower,upper])\n",
    "                    mean_length[lower,upper]=np.mean(lengthdict[lower,upper])\n",
    "                    mean_sents_length[lower,upper]=np.mean(senlengthdict[lower,upper])\n",
    "                    nr_of_starts[lower,upper]=np.nansum(startsdict[lower,upper])\n",
    "                    nr_of_responses[lower,upper]=np.nansum(responsesdict[lower,upper])\n",
    "\n",
    "        # we have collected all data for a single user, and now we do stuff with it:\n",
    "        weekcount = determine_week_activity(first_date,last_date,bindict)   \n",
    "        activity = [len(x) for x in weekcount.values()]\n",
    "        activity_nonempty = [len(x) for x in weekcount.values() if not x==[]]\n",
    "        \n",
    "        print \"The first post: \", first_date\n",
    "        print \"The last post: \", last_date\n",
    "        print \"Activity spread over: \", last_date-first_date\n",
    "        print \"The average nr of posts per week: \", np.mean(activity), \"including long times of inactivity.\"\n",
    "        print \"The average nr of posts in non-empty weeks: \", np.mean(activity_nonempty)\n",
    "        print \"The range of activity: \", min([len(x) for x in weekcount.values()]), \" to \", max([len(x) for x in weekcount.values()]), \" posts per week\"\n",
    "        print\n",
    "        \n",
    "        # extra inzicht\n",
    "        # compare_variables(pp,user,nr_of_posts,mean_quest,mean_object,mean_sent,mean_length,mean_sents_length,nr_of_starts,nr_of_responses)\n",
    "        # compare_two(user,mean_quest,nr_of_responses)\n",
    "        \n",
    "        # maak dataframe van alle variabelen\n",
    "        df = pd.DataFrame({'Que': mean_quest.values(), 'Sub': mean_object.values(), 'Sen': mean_sent.values(),'Len': mean_length.values(), 'Wor': mean_sents_length.values(),'Sta': nr_of_starts.values(),'Res': nr_of_responses.values()}).dropna()\n",
    "\n",
    "        #heatmaps:\n",
    "        #plt.close()\n",
    "        # bepaal correlatiematrix van variabelen\n",
    "        #for m in ['pearson', 'kendall']:\n",
    "            #rho = df.corr(method = m) #dubbel..\n",
    "            #visualise(df, m)\n",
    "        #plt.show()\n",
    "#plt.show()        \n",
    "# show average correlations (per test, and in general)\n",
    "#show_correlations([averagedict,normaldict,nonnormaldict],countnonnormal, rho)\n",
    "\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-11-08 09:36:00  \\  2014-11-08 09:36:00  /  2014-11-15 09:36:00\n",
      "2014-11-15 09:36:00  \\  2014-11-17 09:36:00  /  2014-11-22 09:36:00\n",
      "2014-12-13 09:36:00  \\  2014-12-18 09:36:00  /  2014-12-20 09:36:00\n",
      "2014-12-20 09:36:00  \\  2014-12-26 09:36:00  /  2014-12-27 09:36:00\n"
     ]
    }
   ],
   "source": [
    "#determine nr of posts in a week\n",
    "\n",
    "first_date = datetime.strptime(\"25/10/2014 - 09:36\", '%d/%m/%Y - %H:%M')\n",
    "last_date = first_date + timedelta(days=366)\n",
    "dates = [first_date + timedelta(days=62),first_date + timedelta(days=23),first_date + timedelta(days=14),first_date + timedelta(days=54)]\n",
    "\n",
    "for d in range(0,(last_date-first_date).days,7):\n",
    "    border1 = first_date+timedelta(days=d)\n",
    "    border2 = first_date+timedelta(days=d+7)\n",
    "    for a_date in dates:\n",
    "        if border1<=a_date<border2:\n",
    "            print border1, \" \\ \", a_date, \" / \", border2\n",
    "            # write to dictionary with borders as keys (start dict at first activity, end at last)\n",
    "\n",
    "# determine average length of values in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4) -0.0371477826962\n",
      "(5, 4) -0.0116371475249\n",
      "(5, 0) 0.167089688543\n",
      "(3, 0) 0.0351211230674\n",
      "(5, 2) 0.0491713185558\n",
      "(6, 1) -0.0329269224226\n",
      "(3, 1) 0.0569689490541\n",
      "(3, 2) 0.167349250464\n",
      "(2, 1) 0.247009057214\n",
      "(6, 0) 0.110506895394\n",
      "(6, 3) 0.0591017614612\n",
      "(2, 0) 0.116947043632\n",
      "(6, 2) 0.0647423888313\n",
      "(4, 3) 0.0330315516744\n",
      "(5, 1) 0.0062988958561\n",
      "(4, 2) 0.0566331265763\n",
      "(1, 0) 0.0776204892785\n",
      "(5, 3) 0.18647860511\n",
      "(4, 1) 0.151254831936\n",
      "(6, 5) 0.290548054865\n",
      "(4, 0) 0.0435062984641\n",
      "          Len       Que       Res       Sen       Sta       Sub  Wor\n",
      "Len       NaN       NaN       NaN       NaN       NaN       NaN  NaN\n",
      "Que  0.077620       NaN       NaN       NaN       NaN       NaN  NaN\n",
      "Res  0.116947  0.247009       NaN       NaN       NaN       NaN  NaN\n",
      "Sen  0.035121  0.056969  0.167349       NaN       NaN       NaN  NaN\n",
      "Sta  0.043506  0.151255  0.056633  0.033032       NaN       NaN  NaN\n",
      "Sub  0.167090  0.006299  0.049171  0.186479 -0.011637       NaN  NaN\n",
      "Wor  0.110507 -0.032927  0.064742  0.059102 -0.037148  0.290548  NaN\n"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame().reindex_like(rho)\n",
    "for (i,j) in averagedict:\n",
    "    print (i,j), np.nanmean(averagedict[(i,j)])\n",
    "    frame.iloc[i][j]=np.nanmean(averagedict[(i,j)])\n",
    "print frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
