{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import seaborn as sbn\n",
    "import json, re, time, unicodedata, unidecode, codecs, random, math, warnings\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "from pattern.nl import parse, split, parsetree\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import date, datetime, timedelta\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pattern.nl import sentiment\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import normaltest\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# files\n",
    "#MWE\n",
    "#topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE_topic.json'))\n",
    "#posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE.json'))\n",
    "#regular\n",
    "forums = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-35-45 _amazones_forums_export.json'))\n",
    "topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-36-51_amazones_forum_topics_export.json'))\n",
    "posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_forum_posts_export.json'))\n",
    "users = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_users_export.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove non-ascii characters from file (otherwise they will become part of the tokens)\n",
    "def remove_non_ascii(text):\n",
    "    return unidecode.unidecode(text)\n",
    "    #return ''.join([i if ord(i) < 128 else ' ' for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    #remove all links, images, quotes, and emailaddresses\n",
    "    text=re.sub('<a.*?>(.*?)</a>','',text) #remove links\n",
    "    text=re.sub('(http:|www)\\S*','',text) #remove links without markup\n",
    "    text=re.sub('\\[\\\\\\/url\\]','',text)\n",
    "    text=re.sub('<img.*?/>', '',text) #remove images\n",
    "    text=re.sub('<div class=\"bb-quote\">((\\s|\\S)*?)</div>','',text) #remove quotes\n",
    "    text=re.sub('<script.*?>([\\S\\s]*?)</script>','',text) #remove emailaddresses\n",
    "\n",
    "    #replace all emoticon-icons\n",
    "    text=re.sub('<img.*?title=\"(.*?)\".*?/>', '(EMO:\\\\1)',text) #replace emoticons by textual indicators \n",
    "\n",
    "    # replace (most) sideways latin emoticons\n",
    "    text=re.sub('[^>]:-?(\\)|\\])','(EMO:smiley)',text)\n",
    "    text=re.sub(u'☺️','(EMO:smiley)',text)\n",
    "    text=re.sub('[^>]:-?(\\(|\\[)','(EMO:sad)',text)\n",
    "    text=re.sub(';-?(\\)|\\])','(EMO:wink)',text)\n",
    "    text=re.sub(r'(:|;|x|X)-?(D)+\\b','(EMO:laugh)',text)\n",
    "    text=re.sub(':-?(/|\\\\\\|\\|)','(EMO:frown)',text)\n",
    "    text=re.sub(r'(:|;)-?(p|P)+\\b','(EMO:cheeky)',text)\n",
    "    text=re.sub('(:|;)(\\'|\\\")-?(\\(|\\[)','(EMO:cry)',text)\n",
    "    text=re.sub('\\<3+','(EMO:heart)',text)\n",
    "    text=re.sub(u'❤️','(EMO:heart)',text)\n",
    "    text=re.sub('((\\>:-?(\\(|\\]))|(\\>?:-?@))','(EMO:angry)',text)\n",
    "    text=re.sub('\\>:-?(\\)|\\])','(EMO:evil)',text)\n",
    "    text=re.sub(r'(:|;)-?(O|o|0)+\\b','(EMO:shock)',text)\n",
    "    text=re.sub('(:|;)-?(K|k|x|X)','(EMO:kiss)',text)\n",
    "    # :s\n",
    "    # :x is eigenlijk geen kus, geloof ik...\n",
    "\n",
    "\n",
    "    #other important adjustments:\n",
    "    text=re.sub('m\\'?n\\s','mijn ',text) # replacing m'n and mn with mijn, so it gets parsed correctly.\n",
    "    text=re.sub('z\\'?n\\s','zijn ',text) #replacing z'n and zn with zijn\n",
    "    text=re.sub('d\\'?r\\s','haar ',text) #replacing d'r and dr with zijn (only if followed by space, so dr. stays dr.)\n",
    "\n",
    "    # replace all emoticons (and other things) written between double colons\n",
    "    text=re.sub(':([a-zA-Z]+):','(EMO:\\\\1)',text)\n",
    "\n",
    "    # remove remaining markup\n",
    "    text=re.sub('</?(ol|style|b|p|em|u|i|strong|br|span|div|blockquote|li)(.*?)/?>','',text)\n",
    "    text=re.sub('(\\[|\\]|\\{|\\})', '',text)\n",
    "\n",
    "    # separate text from punctuation (may cause double/triple spaces - does not matter at this point)\n",
    "    text = re.sub('(\\.{2,}|/|\\)|,|!|\\?)','\\\\1 ',text) # space behind\n",
    "    text=re.sub('(/|\\()',' \\\\1',text) # space in front\n",
    "    text=re.sub('(\\w{2,})(\\.|,)','\\\\1 \\\\2 ',text) #space 'between'\n",
    "\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make two dictionaries: user's post T imes, and user's P ost texts.\n",
    "def make_P_T_and_D(topics,posts,count=2500):\n",
    "    P = defaultdict(list)\n",
    "    T = defaultdict(list)\n",
    "    D = []\n",
    "\n",
    "    for t in reversed(topics):    \n",
    "        P[t['Author uid']].append((remove_non_ascii(cleanup(t[\"Body\"])),1))\n",
    "        T[t['Author uid']].append(t['Post date'])\n",
    "        D.append(datetime.strptime(t['Post date'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "        count-=1\n",
    "        if count-1<=0:\n",
    "            break\n",
    "        for p in reversed(posts):\n",
    "            if p['Forum Topic ID'] == t['Nid']:\n",
    "                P[p['Auteur-uid']].append((remove_non_ascii(cleanup(p[\"Body\"])),0))\n",
    "                T[p['Auteur-uid']].append(p['Datum van inzending'])\n",
    "                D.append(datetime.strptime(p['Datum van inzending'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "                count-=1\n",
    "                if count-1<=0:\n",
    "                    break\n",
    "    return (P,T,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_binlist(D,timetick=1): #timetick in days\n",
    "    lower = min(D)\n",
    "    upper = max(D)\n",
    "\n",
    "    if lower.time()>=datetime.strptime('4:00','%H:%M').time():\n",
    "        lower = lower.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        lower = (lower+timedelta(days = -1)).replace(hour=4,minute=0)\n",
    "\n",
    "    if upper.time()<datetime.strptime('12:00','%H:%M').time():\n",
    "        upper = upper.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        upper = (upper+timedelta(days=1)).replace(hour=4,minute=0)\n",
    "\n",
    "        return([lower + timedelta(hours=x) for x in range(0, 24*(upper-lower).days, timetick)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_questionmarks(body):\n",
    "    Q = 0\n",
    "    for sentence in sent_tokenize(body):\n",
    "        if re.search('\\?+', sentence):\n",
    "            Q+=1\n",
    "    if len(sent_tokenize(body))!=0:\n",
    "        return float(Q)/float(len(sent_tokenize(body)))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average sentiment score per sentence in the string, calculated by pattern\n",
    "def determine_sentiment(body):\n",
    "    S = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        S.append(sentiment(sentence)[0])\n",
    "    return np.mean(S)\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average objectivity score per sentence in the string, calculated by pattern\n",
    "def determine_objectivity(body):\n",
    "    O = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        O.append(sentiment(sentence)[1])\n",
    "    return np.mean(O)\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the total nr of sentences the string is built of\n",
    "def determine_length(body):\n",
    "    # in sentences:\n",
    "    return(len(sent_tokenize(body)))\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average nr of words in sentences that occur in the string\n",
    "def determine_sent_length(body):\n",
    "    # in words:\n",
    "    L = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        L.append(len(word_tokenize(sentence)))\n",
    "    return np.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_variables(pp,user,nr_of_posts,mean_quest,mean_object,mean_sents,mean_length,mean_sents_length,nr_of_starts,nr_of_responses):\n",
    "    fig = plt.figure(1)\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    posts, = ax.plot(nr_of_posts.values(),'b.', label = 'nr of posts', alpha = 0.5) #blue\n",
    "    senlength, = ax.plot(mean_sents_length.values(),'k.', label = 'mean sentence length (words)',alpha = 0.5) #yellow\n",
    "    postlength, = ax.plot(mean_length.values(),'c.', label = 'mean post length (sentences)',alpha = 0.5) #cyan\n",
    "    starts, = ax.plot(nr_of_starts.values(),'g.', label = 'start posts', alpha = 0.5) #black\n",
    "    #responses, = ax.plot(nr_of_responses.values(),'y.', label = 'response posts', alpha = 0.5)\n",
    "    \n",
    "    first_legend = plt.legend(handles=[posts,senlength,postlength,starts], title = \"left axis\", loc='upper left', bbox_to_anchor=(0, -0.1),ncol=1)\n",
    "    axx = plt.gca().add_artist(first_legend)\n",
    "    \n",
    "    ax1 = ax.twinx()\n",
    "    qmarks, = ax1.plot(mean_quest.values(),'y.', label = 'question ratio',alpha = 0.5) #green\n",
    "    ovalues, = ax1.plot(mean_object.values(),'r.', label = 'subjectivity',alpha = 0.5) #red\n",
    "    svalues, = ax1.plot(mean_sent.values(),'m.', label = 'sentiment',alpha = 0.5) #magenta\n",
    "    \n",
    "    \n",
    "    second_legend = plt.legend(handles=[qmarks,ovalues,svalues], title = \"right axis\",loc='upper right', bbox_to_anchor=(1, -0.1),ncol=1)\n",
    "\n",
    "    ax.set_ylim(0,40)\n",
    "    ax1.set_ylim(-1,1)\n",
    "    \n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height, box.width, box.height * 0.7])\n",
    "    box = ax1.get_position()\n",
    "    ax1.set_position([box.x0, box.y0 + box.height, box.width, box.height * 0.7])\n",
    "    \n",
    "    plt.title(user)\n",
    "    plt.show()\n",
    "    pp.savefig(fig, dpi = 300, transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_two(user,x,y):\n",
    "    x = x.values()\n",
    "    y = y.values()\n",
    "    \n",
    "    plt.plot(x,y, 'bo', alpha = 0.1)\n",
    "    plt.xlabel('x-axis variable')\n",
    "    plt.ylabel('y-axis variable')\n",
    "    plt.xlim(0,30)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.title(user)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determine which variables are non-normal, and make a mask for correlations that are measured nonparametrically\n",
    "def mask_nonnormal(df,mask):\n",
    "    for column in df:\n",
    "        index = df.columns.get_loc(column)\n",
    "        z,p = normaltest(df[column].tolist())\n",
    "        # als de data in de kolom significant niet-normaal verdeeld is, activeer dan het maskeer voor de relevante rij en kolom\n",
    "        if p<0.05:\n",
    "            mask[index]=True #rijen blokkeren\n",
    "            mask[0::1,index]=True #kolommen blokkeren\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualise(df, method):\n",
    "    global averagedict\n",
    "    rho = df.corr(method = m)\n",
    "    mask = np.zeros_like(rho, dtype=np.bool)\n",
    "    \n",
    "    if method == 'pearson' or method == 'spearman':\n",
    "        mask = mask_nonnormal(df,mask)\n",
    "        cmap = sbn.diverging_palette(120, 260, n=21, s=80)\n",
    "        cbar_kws= dict(use_gridspec=False,location=\"left\",label= \"Pearson's Rho\")\n",
    "    else:\n",
    "        mask = ~mask_nonnormal(df,mask)\n",
    "        cmap = sbn.diverging_palette(10, 30, n=21, s=99, l=65)\n",
    "        cbar_kws = dict(use_gridspec=False,location=\"right\",label= \"Kendall's Tau\")\n",
    "\n",
    "    #mask the upper half of the figure\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "    # calculate averages        \n",
    "    for i,row in enumerate(mask):\n",
    "        for j,column in enumerate(row):\n",
    "            if column == False: # all unmasked positions\n",
    "                averagedict[i,j].append(rho.iloc[i][j])\n",
    "    \n",
    "    # make heatmap\n",
    "    ax = sbn.heatmap(rho, mask=mask, cbar_kws = cbar_kws, cmap=cmap, vmin = -1, vmax = 1, annot = True, fmt='1.2f')\n",
    "    plt.title('results of (non-)parametric correlation tests')\n",
    "    #plt.savefig('seabornPandas.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PTD = make_P_T_and_D(topics,posts) \n",
    "P = PTD[0]\n",
    "T = PTD[1]\n",
    "D = PTD[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/307 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\python27\\src\\tqdm\\tqdm\\_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "\n",
      "  3%|██▏                                                                               | 8/307 [00:50<31:34,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|██████▎                                                                          | 24/307 [03:05<36:27,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█████████▏                                                                       | 35/307 [03:50<29:47,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|██████████▌                                                                      | 40/307 [04:36<30:47,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|████████████▉                                                                    | 49/307 [05:39<29:46,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|███████████████▌                                                                 | 59/307 [06:40<28:04,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-557fd49a8408>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;31m#1b) put all user's posts in a dictionary (keyed by the time bin boundaries)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%d/%m/%Y - %H:%M'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                         \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\_strptime.pyc\u001b[0m in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;34m\"\"\"Return a time struct based on the input string and the format string.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_TimeRE_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_regex_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_cache_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m         \u001b[0mlocale_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_TimeRE_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocale_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         if (_getlang() != locale_time.lang or\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plt.close()\n",
    "binlist = make_binlist(D,1) #timetick in (whole) days\n",
    "pp = PdfPages(\"plots-author-BVN.pdf\")\n",
    "normal = pd.DataFrame()\n",
    "nonnormal = pd.DataFrame()\n",
    "averagedict = defaultdict(list)\n",
    "\n",
    "for user in tqdm(T):\n",
    "    if len(T[user])<30:\n",
    "        pass # go to next user. This one has too little activity\n",
    "    else:\n",
    "        print user\n",
    "        csv_date = []\n",
    "        csv_sentiment = []\n",
    "        csv_questionmarks = []\n",
    "        csv_subjectivity = []\n",
    "        csv_sentencelength = []\n",
    "        csv_postlength = []\n",
    "        csv_startposts = []\n",
    "        csv_endposts = []\n",
    "        csv_inactivity = []\n",
    "        \n",
    "        # assume an inactivity of 0 time bins\n",
    "        inactivity = 0\n",
    "        \n",
    "        # variables\n",
    "        bindict = defaultdict(list)\n",
    "        postdict = defaultdict(list)\n",
    "        metadict = defaultdict(list)\n",
    "\n",
    "        # length of posts (in words, or sentences)\n",
    "        # nr of replies to posts vs nr of starting posts\n",
    "        # linguistic markers, like adjectives / pronouns / emoticons, and the diversity of topics / vocabulary\n",
    "\n",
    "\n",
    "        # 1) look through the sorted list of datetimes\n",
    "        for index,boundary in enumerate(binlist):\n",
    "            if index+1>=len(binlist):\n",
    "                break\n",
    "            else:\n",
    "                # 1a) set time bin boundaries\n",
    "                lower = binlist[index]\n",
    "                upper = binlist[index+1]\n",
    "\n",
    "                #1b) put all user's posts in a dictionary (keyed by the time bin boundaries) \n",
    "                for time in T[user]:\n",
    "                    if lower<=datetime.strptime(time, '%d/%m/%Y - %H:%M')<upper:\n",
    "                        body = P[user][T[user].index(time)][0]\n",
    "                        meta = P[user][T[user].index(time)][1]\n",
    "                        #print time, meta, body\n",
    "\n",
    "                        bindict[lower,upper].append(time)\n",
    "                        postdict[lower,upper].append(body) \n",
    "                        metadict[lower,upper].append(meta)\n",
    "\n",
    "            # fill up empty places in dictionary\n",
    "            if len(bindict[lower,upper])==0:\n",
    "                bindict[lower,upper]=[]\n",
    "                postdict[lower,upper]=[]\n",
    "                metadict[lower,upper]=[]\n",
    "\n",
    "        #convert dictionaries to single values per bin, for ML purposes\n",
    "            body = '. '.join(postdict[lower,upper]) #can be empty!\n",
    "            csv_date.append(lower)\n",
    "            \n",
    "            # when no post was posted within the boundaries of this bin, import nan value and add one to the inactivity variable\n",
    "            if len(body) == 0:\n",
    "                csv_sentiment.append(float('nan'))\n",
    "                csv_questionmarks.append(float('nan'))\n",
    "                csv_subjectivity.append(float('nan'))\n",
    "                csv_sentencelength.append(float('nan'))\n",
    "                csv_postlength.append(float('nan'))\n",
    "                csv_startposts.append(float('nan'))\n",
    "                csv_endposts.append(float('nan'))\n",
    "                # add one time bin to inactivity level\n",
    "                inactivity+=1\n",
    "                csv_inactivity.append(float('nan'))\n",
    "            else:\n",
    "                csv_sentiment.append(determine_sentiment(body))\n",
    "                csv_questionmarks.append(determine_questionmarks(body))\n",
    "                csv_subjectivity.append(determine_objectivity(body))\n",
    "                csv_sentencelength.append(determine_sent_length(body))\n",
    "                csv_postlength.append(np.mean([determine_length(x) for x in postdict[lower,upper]]))\n",
    "                csv_startposts.append(np.mean(metadict[lower,upper]))\n",
    "                csv_endposts.append(1-np.mean(metadict[lower,upper]))\n",
    "                csv_inactivity.append(inactivity)\n",
    "                # set inactivity level back to 0 time bins\n",
    "                inactivity = 0\n",
    "\n",
    "        # maak dataframe van alle variabelen\n",
    "        df = pd.DataFrame({\"Date & Time\": csv_date, \"Sentiment\": csv_sentiment, \"Questions\": csv_questionmarks, \n",
    "                           \"Subjectivity\": csv_subjectivity, \"Words/Sentence\": csv_sentencelength, \n",
    "                           \"Sentences/Post\": csv_postlength, \"First posts\": csv_startposts,\n",
    "                           \"Follow-up posts\": csv_endposts, \"Inactivity\": csv_inactivity})\n",
    "        df.to_csv(\"features_user_\"+str(user)+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this text is between brackets.',\n",
       " 'This one as well.',\n",
       " 'And this text is from another post']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"this text is between brackets. This one as well\",\"And this text is from another post\"]\n",
    "sent_tokenize('. '.join(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4) -0.0371477826962\n",
      "(5, 4) -0.0116371475249\n",
      "(5, 0) 0.167089688543\n",
      "(3, 0) 0.0351211230674\n",
      "(5, 2) 0.0491713185558\n",
      "(6, 1) -0.0329269224226\n",
      "(3, 1) 0.0569689490541\n",
      "(3, 2) 0.167349250464\n",
      "(2, 1) 0.247009057214\n",
      "(6, 0) 0.110506895394\n",
      "(6, 3) 0.0591017614612\n",
      "(2, 0) 0.116947043632\n",
      "(6, 2) 0.0647423888313\n",
      "(4, 3) 0.0330315516744\n",
      "(5, 1) 0.0062988958561\n",
      "(4, 2) 0.0566331265763\n",
      "(1, 0) 0.0776204892785\n",
      "(5, 3) 0.18647860511\n",
      "(4, 1) 0.151254831936\n",
      "(6, 5) 0.290548054865\n",
      "(4, 0) 0.0435062984641\n",
      "          Len       Que       Res       Sen       Sta       Sub  Wor\n",
      "Len       NaN       NaN       NaN       NaN       NaN       NaN  NaN\n",
      "Que  0.077620       NaN       NaN       NaN       NaN       NaN  NaN\n",
      "Res  0.116947  0.247009       NaN       NaN       NaN       NaN  NaN\n",
      "Sen  0.035121  0.056969  0.167349       NaN       NaN       NaN  NaN\n",
      "Sta  0.043506  0.151255  0.056633  0.033032       NaN       NaN  NaN\n",
      "Sub  0.167090  0.006299  0.049171  0.186479 -0.011637       NaN  NaN\n",
      "Wor  0.110507 -0.032927  0.064742  0.059102 -0.037148  0.290548  NaN\n"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame().reindex_like(rho)\n",
    "for (i,j) in averagedict:\n",
    "    print (i,j), np.nanmean(averagedict[(i,j)])\n",
    "    frame.iloc[i][j]=np.nanmean(averagedict[(i,j)])\n",
    "print frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   time  var1  var2  var3\n",
      "4     4   0.7   9.0   0.4\n",
      "5     5   0.4   8.0   0.2\n",
      "7     7   0.1   7.0   0.8\n",
      "['time', 'var1', 'var2', 'var3']\n",
      "['time', 'var2', 'var1', 'var3']\n",
      "   time  var2  var1  var3\n",
      "4     4   9.0   0.7   0.4\n",
      "5     5   8.0   0.4   0.2\n",
      "7     7   7.0   0.1   0.8\n",
      "0.7 4\n",
      "0.4 0\n",
      "0.1 1\n"
     ]
    }
   ],
   "source": [
    "def swap_to_front(header, dependent_variable):\n",
    "    header.remove(dependent_variable)\n",
    "    header.insert(1,dependent_variable)\n",
    "    return header\n",
    "\n",
    "#-------------------------------------------------\n",
    "\n",
    "n = float('nan')\n",
    "time = [0,1,2,3,4,5,6,7,8,9]\n",
    "var1 = [n,n,n,n,0.7,0.4,n,0.1,n,n]\n",
    "var2 = [n,n,n,n,9,8,n,7,n,n]\n",
    "var3 = [n,n,n,n,0.4,0.2,n,0.8,n,n]\n",
    "\n",
    "df = pd.DataFrame({\"time\": time, \"var1\": var1, \"var2\":var2, \"var3\": var3}).dropna()\n",
    "print df\n",
    "\n",
    "header = df.columns.tolist()\n",
    "header = swap_to_front(header,\"var2\") # kan gemaakt worden met command line trucje\n",
    "\n",
    "df = df.reindex(columns = header)\n",
    "print df\n",
    "\n",
    "inactivity = 0\n",
    "for i,v in enumerate(var1):\n",
    "    if np.isnan(v):\n",
    "        inactivity+=1\n",
    "    else:\n",
    "        print v, inactivity\n",
    "        inactivity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n",
      "[6, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a.remove(2)\n",
    "print a\n",
    "a.insert(0,6)\n",
    "print a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
