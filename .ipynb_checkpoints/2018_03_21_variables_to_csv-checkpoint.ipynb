{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import seaborn as sbn\n",
    "import json, re, time, unicodedata, unidecode, codecs, random, math, warnings, itertools, os\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "from pattern.nl import parse, split, parsetree\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import date, datetime, timedelta\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pattern.nl import sentiment\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import normaltest\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.19.6'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# files\n",
    "#MWE\n",
    "#topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE_topic.json'))\n",
    "#posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE.json'))\n",
    "#regular\n",
    "forums = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-35-45 _amazones_forums_export.json'))\n",
    "# zorg voor een nieuwe versie van dit bestand; verkeerd opgeslagen dus je mist het kontje!\n",
    "topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-36-51_amazones_forum_topics_export.json'))\n",
    "posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_forum_posts_export.json'))\n",
    "users = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_users_export.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove non-ascii characters from file (otherwise they will become part of the tokens)\n",
    "def remove_non_ascii(text):\n",
    "    return unidecode.unidecode(text)\n",
    "    #return ''.join([i if ord(i) < 128 else ' ' for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    #remove all links, images, quotes, and emailaddresses\n",
    "    text=re.sub('<a.*?>(.*?)</a>','',text) #remove links\n",
    "    text=re.sub('(http:|www)\\S*','',text) #remove links without markup\n",
    "    text=re.sub('\\[\\\\\\/url\\]','',text)\n",
    "    text=re.sub('<img.*?/>', '',text) #remove images\n",
    "    text=re.sub('<div class=\"bb-quote\">((\\s|\\S)*?)</div>','',text) #remove quotes\n",
    "    text=re.sub('<script.*?>([\\S\\s]*?)</script>','',text) #remove emailaddresses\n",
    "\n",
    "    #replace all emoticon-icons\n",
    "    text=re.sub('<img.*?title=\"(.*?)\".*?/>', '(EMO:\\\\1)',text) #replace emoticons by textual indicators \n",
    "\n",
    "    # replace (most) sideways latin emoticons\n",
    "    text=re.sub('[^>]:-?(\\)|\\])','(EMO:smiley)',text)\n",
    "    text=re.sub(u'☺️','(EMO:smiley)',text)\n",
    "    text=re.sub('[^>]:-?(\\(|\\[)','(EMO:sad)',text)\n",
    "    text=re.sub(';-?(\\)|\\])','(EMO:wink)',text)\n",
    "    text=re.sub(r'(:|;|x|X)-?(D)+\\b','(EMO:laugh)',text)\n",
    "    text=re.sub(':-?(/|\\\\\\|\\|)','(EMO:frown)',text)\n",
    "    text=re.sub(r'(:|;)-?(p|P)+\\b','(EMO:cheeky)',text)\n",
    "    text=re.sub('(:|;)(\\'|\\\")-?(\\(|\\[)','(EMO:cry)',text)\n",
    "    text=re.sub('\\<3+','(EMO:heart)',text)\n",
    "    text=re.sub(u'❤️','(EMO:heart)',text)\n",
    "    text=re.sub('((\\>:-?(\\(|\\]))|(\\>?:-?@))','(EMO:angry)',text)\n",
    "    text=re.sub('\\>:-?(\\)|\\])','(EMO:evil)',text)\n",
    "    text=re.sub(r'(:|;)-?(O|o|0)+\\b','(EMO:shock)',text)\n",
    "    text=re.sub('(:|;)-?(K|k|x|X)','(EMO:kiss)',text)\n",
    "    # :s\n",
    "    # :x is eigenlijk geen kus, geloof ik...\n",
    "\n",
    "\n",
    "    #other important adjustments:\n",
    "    text=re.sub('m\\'?n\\s','mijn ',text) # replacing m'n and mn with mijn, so it gets parsed correctly.\n",
    "    text=re.sub('z\\'?n\\s','zijn ',text) #replacing z'n and zn with zijn\n",
    "    text=re.sub('d\\'?r\\s','haar ',text) #replacing d'r and dr with zijn (only if followed by space, so dr. stays dr.)\n",
    "\n",
    "    # replace all emoticons (and other things) written between double colons\n",
    "    text=re.sub(':([a-zA-Z]+):','(EMO:\\\\1)',text)\n",
    "\n",
    "    # remove remaining markup\n",
    "    text=re.sub('</?(ol|style|b|p|em|u|i|strong|br|span|div|blockquote|li)(.*?)/?>','',text)\n",
    "    text=re.sub('(\\[|\\]|\\{|\\})', '',text)\n",
    "\n",
    "    # separate text from punctuation (may cause double/triple spaces - does not matter at this point)\n",
    "    text = re.sub('(\\.{2,}|/|\\)|,|!|\\?)','\\\\1 ',text) # space behind\n",
    "    text=re.sub('(/|\\()',' \\\\1',text) # space in front\n",
    "    text=re.sub('(\\w{2,})(\\.|,)','\\\\1 \\\\2 ',text) #space 'between'\n",
    "\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make two dictionaries: user's post T imes, and user's P ost texts.\n",
    "def make_P_T_and_D(topics,posts):\n",
    "    P = defaultdict(list)\n",
    "    T = defaultdict(list)\n",
    "    D = []\n",
    "\n",
    "    with tqdm(total=len(topics)) as pbar:\n",
    "        for t in reversed(topics):\n",
    "            pbar.update(1)\n",
    "            P[t['Author uid']].append((remove_non_ascii(cleanup(t[\"Body\"])),1))\n",
    "            T[t['Author uid']].append(t['Post date'])\n",
    "            D.append(datetime.strptime(t['Post date'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "    with tqdm(total=len(posts)) as pbar:\n",
    "        for p in reversed(posts):\n",
    "            pbar.update(1)\n",
    "            P[p['Auteur-uid']].append((remove_non_ascii(cleanup(p[\"Body\"])),0))\n",
    "            T[p['Auteur-uid']].append(p['Datum van inzending'])\n",
    "            D.append(datetime.strptime(p['Datum van inzending'], '%d/%m/%Y - %H:%M'))\n",
    "\n",
    "    return (P,T,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_binlist(D,timetick=1): #timetick in days\n",
    "    lower = min(D)\n",
    "    upper = max(D)\n",
    "\n",
    "    if lower.time()>=datetime.strptime('4:00','%H:%M').time():\n",
    "        lower = lower.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        lower = (lower+timedelta(days = -1)).replace(hour=4,minute=0)\n",
    "\n",
    "    if upper.time()<datetime.strptime('12:00','%H:%M').time():\n",
    "        upper = upper.replace(hour = 4, minute = 0)\n",
    "    else:\n",
    "        upper = (upper+timedelta(days=1)).replace(hour=4,minute=0)\n",
    "\n",
    "    return([lower + timedelta(hours=x) for x in range(0, 24*(upper-lower).days, timetick)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_questionmarks(body):\n",
    "    Q = 0\n",
    "    for sentence in sent_tokenize(body):\n",
    "        if re.search('\\?+', sentence):\n",
    "            Q+=1\n",
    "    if len(sent_tokenize(body))!=0:\n",
    "        return float(Q)/float(len(sent_tokenize(body)))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average sentiment score per sentence in the string, calculated by pattern\n",
    "def determine_sentiment(body):\n",
    "    S = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        S.append(sentiment(sentence)[0])\n",
    "    return np.mean(S)\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average objectivity score per sentence in the string, calculated by pattern\n",
    "def determine_objectivity(body):\n",
    "    O = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        O.append(sentiment(sentence)[1])\n",
    "    return np.mean(O)\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the total nr of sentences the string is built of\n",
    "def determine_length(body):\n",
    "    # in sentences:\n",
    "    return(len(sent_tokenize(body)))\n",
    "\n",
    "# expects a string\n",
    "# returns an int, representing the average nr of words in sentences that occur in the string\n",
    "def determine_sent_length(body):\n",
    "    # in words:\n",
    "    L = []\n",
    "    for sentence in sent_tokenize(body):\n",
    "        L.append(len(word_tokenize(sentence)))\n",
    "    return np.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_variables(pp,user,nr_of_posts,mean_quest,mean_object,mean_sents,mean_length,mean_sents_length,nr_of_starts,nr_of_responses):\n",
    "    fig = plt.figure(1)\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    posts, = ax.plot(nr_of_posts.values(),'b.', label = 'nr of posts', alpha = 0.5) #blue\n",
    "    senlength, = ax.plot(mean_sents_length.values(),'k.', label = 'mean sentence length (words)',alpha = 0.5) #yellow\n",
    "    postlength, = ax.plot(mean_length.values(),'c.', label = 'mean post length (sentences)',alpha = 0.5) #cyan\n",
    "    starts, = ax.plot(nr_of_starts.values(),'g.', label = 'start posts', alpha = 0.5) #black\n",
    "    #responses, = ax.plot(nr_of_responses.values(),'y.', label = 'response posts', alpha = 0.5)\n",
    "    \n",
    "    first_legend = plt.legend(handles=[posts,senlength,postlength,starts], title = \"left axis\", loc='upper left', bbox_to_anchor=(0, -0.1),ncol=1)\n",
    "    axx = plt.gca().add_artist(first_legend)\n",
    "    \n",
    "    ax1 = ax.twinx()\n",
    "    qmarks, = ax1.plot(mean_quest.values(),'y.', label = 'question ratio',alpha = 0.5) #green\n",
    "    ovalues, = ax1.plot(mean_object.values(),'r.', label = 'subjectivity',alpha = 0.5) #red\n",
    "    svalues, = ax1.plot(mean_sent.values(),'m.', label = 'sentiment',alpha = 0.5) #magenta\n",
    "    \n",
    "    \n",
    "    second_legend = plt.legend(handles=[qmarks,ovalues,svalues], title = \"right axis\",loc='upper right', bbox_to_anchor=(1, -0.1),ncol=1)\n",
    "\n",
    "    ax.set_ylim(0,40)\n",
    "    ax1.set_ylim(-1,1)\n",
    "    \n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height, box.width, box.height * 0.7])\n",
    "    box = ax1.get_position()\n",
    "    ax1.set_position([box.x0, box.y0 + box.height, box.width, box.height * 0.7])\n",
    "    \n",
    "    plt.title(user)\n",
    "    plt.show()\n",
    "    pp.savefig(fig, dpi = 300, transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_two(user,x,y):\n",
    "    x = x.values()\n",
    "    y = y.values()\n",
    "    \n",
    "    plt.plot(x,y, 'bo', alpha = 0.1)\n",
    "    plt.xlabel('x-axis variable')\n",
    "    plt.ylabel('y-axis variable')\n",
    "    plt.xlim(0,30)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.title(user)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determine which variables are non-normal, and make a mask for correlations that are measured nonparametrically\n",
    "def mask_nonnormal(df,mask):\n",
    "    for column in df:\n",
    "        index = df.columns.get_loc(column)\n",
    "        z,p = normaltest(df[column].tolist())\n",
    "        # als de data in de kolom significant niet-normaal verdeeld is, activeer dan het maskeer voor de relevante rij en kolom\n",
    "        if p<0.05:\n",
    "            mask[index]=True #rijen blokkeren\n",
    "            mask[0::1,index]=True #kolommen blokkeren\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualise(df, method):\n",
    "    global averagedict\n",
    "    rho = df.corr(method = m)\n",
    "    mask = np.zeros_like(rho, dtype=np.bool)\n",
    "    \n",
    "    if method == 'pearson' or method == 'spearman':\n",
    "        mask = mask_nonnormal(df,mask)\n",
    "        cmap = sbn.diverging_palette(120, 260, n=21, s=80)\n",
    "        cbar_kws= dict(use_gridspec=False,location=\"left\",label= \"Pearson's Rho\")\n",
    "    else:\n",
    "        mask = ~mask_nonnormal(df,mask)\n",
    "        cmap = sbn.diverging_palette(10, 30, n=21, s=99, l=65)\n",
    "        cbar_kws = dict(use_gridspec=False,location=\"right\",label= \"Kendall's Tau\")\n",
    "\n",
    "    #mask the upper half of the figure\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "    # calculate averages        \n",
    "    for i,row in enumerate(mask):\n",
    "        for j,column in enumerate(row):\n",
    "            if column == False: # all unmasked positions\n",
    "                averagedict[i,j].append(rho.iloc[i][j])\n",
    "                if method == 'pearson' or method == 'spearman':\n",
    "                    normaldict[i,j].append(rho.iloc[i][j])\n",
    "                else:\n",
    "                    nonnormaldict[i,j].append(rho.iloc[i][j])\n",
    "                    countnonnormal[i,j].append(1)\n",
    "    \n",
    "    # make heatmap\n",
    "    ax = sbn.heatmap(rho, mask=mask, cbar_kws = cbar_kws, cmap=cmap, vmin = -1, vmax = 1, annot = True, fmt='1.2f')\n",
    "    plt.title('results of (non-)parametric correlation tests')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_correlations(dictionaries,countnonnormal,rho):\n",
    "    for d in dictionaries:\n",
    "        frame = pd.DataFrame().reindex_like(rho)\n",
    "        for (i,j) in d:\n",
    "            frame.iloc[i][j]=np.nanmean(d[(i,j)])\n",
    "        print frame\n",
    "\n",
    "    frame = pd.DataFrame().reindex_like(rho)\n",
    "    for (i,j) in countnonnormal:\n",
    "        frame.iloc[i][j]=np.nansum(countnonnormal[(i,j)])\n",
    "    print frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_week_activity(first_date,last_date,bindict):\n",
    "    for d in range(0, (last_date-first_date).days,7):\n",
    "        week_start = first_date+timedelta(days=d)\n",
    "        week_end = first_date+timedelta(days=d+7)\n",
    "        for date in list(itertools.chain.from_iterable(bindict.values())):\n",
    "            if week_start<=datetime.strptime(date, '%d/%m/%Y - %H:%M')<week_end:\n",
    "                weekcountdict[week_start,week_end].append(1)\n",
    "        if len(weekcountdict[week_start,week_end])==0:\n",
    "            weekcountdict[week_start,week_end] = []\n",
    "    return weekcountdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_past_activity(bindict,index,hours_back=24):\n",
    "    if 0<= index-hours_back<=len(bindict):\n",
    "        past_activity = np.sum([len(bindict[binlist[index-x],binlist[index+1-x]]) for x in range(hours_back)])\n",
    "    else:\n",
    "        past_activity=0\n",
    "    return past_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def print_information(user, T, first_date,last_date,activity,activity_nonempty,weekcount):\n",
    "    print \"User:\", user\n",
    "    print \"posted one or more posts in\", len(T[user]), \"'bins'.\" \n",
    "    print \"The first post: \", first_date\n",
    "    print \"The last post: \", last_date\n",
    "    print \"Activity spread over: \", last_date-first_date\n",
    "    print \"The average nr of posts per week: \", np.mean(activity), \"including long times of inactivity.\"\n",
    "    print \"The average nr of posts in non-empty weeks: \", np.mean(activity_nonempty)\n",
    "    print \"The range of activity: \", min([len(x) for x in weekcount.values()]), \" to \", max([len(x) for x in weekcount.values()]), \" posts per week\"\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ec28b929734adc955d63dad63c286b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5ed490dda449f3bc60436cfa82b218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PTD = make_P_T_and_D(topics,posts) \n",
    "P = PTD[0]\n",
    "T = PTD[1]\n",
    "D = PTD[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f882bffe4c844999ebafec1d260904d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0866f9ca1c47b8acf4987d1b12532b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\python27\\src\\tqdm\\tqdm\\_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2314"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08348058f2fd4a63959122181b227b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2310"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fa7bf24d104e2696f053cb756bbc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3779"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b3c4277049447f807d8d86fcc67994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2440"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491f2b163dc940e18cbdf6692207a23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-74549bac9e59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[1;31m#update one in tqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%d/%m/%Y - %H:%M'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mfirst_date\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                             \u001b[0mfirst_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%d/%m/%Y - %H:%M'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sternheimam\\AppData\\Local\\Continuum\\anaconda2\\lib\\_strptime.pyc\u001b[0m in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0mweekday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjulian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[0mfound_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mgroup_key\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfound_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m         \u001b[1;31m# Directives not explicitly handled below:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;31m#   c, x, X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plt.close()\n",
    "path = r\"C:\\Users\\sternheimam\\Desktop\\my-notebook\\user-csvs\" \n",
    "\n",
    "binlist = make_binlist(D,1) #timetick in (whole) days\n",
    "pp = PdfPages(\"plots-author-BVN.pdf\")\n",
    "normaldict = defaultdict(list)\n",
    "nonnormaldict = defaultdict(list)\n",
    "averagedict = defaultdict(list)\n",
    "countnonnormal = defaultdict(list)\n",
    "over_treshold = []\n",
    "neg_diff = 24\n",
    "\n",
    "# first list all usable users (enables nice progress bar)\n",
    "for user in T:\n",
    "    if len(T[user])<30:\n",
    "        pass # go to next user. This one has too little activity\n",
    "    else:\n",
    "        over_treshold.append(user)\n",
    "        \n",
    "with tqdm(total=len(over_treshold)) as processbar:\n",
    "    for user in over_treshold:\n",
    "        print user,\n",
    "        processbar.update(1)\n",
    "        \n",
    "        first_date = 0\n",
    "        last_date = 0\n",
    "        \n",
    "        csv_date = []\n",
    "        csv_sentiment = []\n",
    "        csv_questionmarks = []\n",
    "        csv_subjectivity = []\n",
    "        csv_sentencelength = []\n",
    "        csv_postlength = []\n",
    "        csv_startposts = []\n",
    "        csv_inactivity = []\n",
    "        csv_backtrack = []\n",
    "        \n",
    "        # assume an inactivity of 0 time bins\n",
    "        inactivity = 0\n",
    "        \n",
    "        # variables\n",
    "        bindict = defaultdict(list)\n",
    "        postdict = defaultdict(list)\n",
    "        metadict = defaultdict(list)\n",
    "        weekcountdict = defaultdict(list)\n",
    "        backtrackdict = defaultdict(list)\n",
    "        \n",
    "        # linguistic markers, like adjectives / pronouns / emoticons, and the diversity of topics / vocabulary\n",
    "        # time relative posts (see email from stephan)\n",
    "\n",
    "        # 1) look through the sorted list of datetimes\n",
    "        for index,boundary in enumerate(tqdm(binlist)):\n",
    "            if index+1>=len(binlist):\n",
    "                break\n",
    "            else:\n",
    "                # 1a) set time bin boundaries\n",
    "                lower = binlist[index]\n",
    "                upper = binlist[index+1]\n",
    "                \n",
    "                #1b) put all user's posts in a dictionary (keyed by the time bin boundaries) \n",
    "                for time in T[user]:\n",
    "                    #update one in tqdm\n",
    "                    if lower<=datetime.strptime(time, '%d/%m/%Y - %H:%M')<upper:\n",
    "                        if first_date == 0:\n",
    "                            first_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                            last_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                        else:\n",
    "                            last_date = datetime.strptime(time, '%d/%m/%Y - %H:%M')\n",
    "                                                  \n",
    "                        body = P[user][T[user].index(time)][0]\n",
    "                        meta = P[user][T[user].index(time)][1]\n",
    "\n",
    "                        bindict[lower,upper].append(time)\n",
    "                        past_activity = determine_past_activity(bindict,index,24)\n",
    "                        \n",
    "                        postdict[lower,upper].append(body) \n",
    "                        metadict[lower,upper].append(meta)\n",
    "                        backtrackdict[lower,upper].append(past_activity)\n",
    "                        \n",
    "                # fill up empty places in dictionary\n",
    "                if len(bindict[lower,upper])==0:\n",
    "                    bindict[lower,upper]=[]\n",
    "                    postdict[lower,upper]=[]\n",
    "                    metadict[lower,upper]=[]\n",
    "                    backtrackdict[lower,upper]=[]\n",
    "\n",
    "                #convert dictionaries to single values per bin, for ML purposes\n",
    "                body = '. '.join(postdict[lower,upper]) #can be empty!\n",
    "                csv_date.append(lower)\n",
    "            \n",
    "                # when no post was posted within the boundaries of this bin, import nan value and add one to the inactivity variable\n",
    "                if len(body) == 0:\n",
    "                    csv_sentiment.append(float('nan'))\n",
    "                    csv_questionmarks.append(float('nan'))\n",
    "                    csv_subjectivity.append(float('nan'))\n",
    "                    csv_sentencelength.append(float('nan'))\n",
    "                    csv_postlength.append(float('nan'))\n",
    "                    csv_startposts.append(float('nan'))\n",
    "                    inactivity+=1\n",
    "                    csv_inactivity.append(float('nan'))\n",
    "                    csv_backtrack.append(float('nan'))\n",
    "                else:\n",
    "                    csv_sentiment.append(determine_sentiment(body))\n",
    "                    csv_questionmarks.append(determine_questionmarks(body))\n",
    "                    csv_subjectivity.append(determine_objectivity(body))\n",
    "                    csv_sentencelength.append(determine_sent_length(body))\n",
    "                    csv_postlength.append(np.mean([determine_length(x) for x in postdict[lower,upper]]))\n",
    "                    csv_startposts.append(np.mean(metadict[lower,upper]))\n",
    "                    csv_inactivity.append(inactivity)\n",
    "                    csv_backtrack.append(np.sum(backtrackdict[lower,upper][-1]))\n",
    "                    inactivity = 0\n",
    "\n",
    "        # we have collected all data for a single user, and now we do stuff with it:\n",
    "        weekcount = determine_week_activity(first_date,last_date,bindict)   \n",
    "        activity = [len(x) for x in weekcount.values()]\n",
    "        activity_nonempty = [len(x) for x in weekcount.values() if not x==[]]\n",
    "        #print_information(user, T, first_date,last_date,activity,activity_nonempty,weekcount)            \n",
    "                \n",
    "        # maak dataframe van alle variabelen\n",
    "        df = pd.DataFrame({\"Date & Time\": csv_date, \"Sentiment\": csv_sentiment, \"Questions\": csv_questionmarks, \n",
    "                           \"Subjectivity\": csv_subjectivity, \"Words/Sentence\": csv_sentencelength, \n",
    "                           \"Sentences/Post\": csv_postlength, \"First posts\": csv_startposts,\n",
    "                           \"Inactivity\": csv_inactivity, \"posts in last 24H\": csv_backtrack}).dropna()\n",
    "        name = \"features_user_\"+str(user)+\".csv\"       \n",
    "        df.to_csv(os.path.join(path,name),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1>=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(above30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-11-08 09:36:00  \\  2014-11-08 09:36:00  /  2014-11-15 09:36:00\n",
      "2014-11-15 09:36:00  \\  2014-11-17 09:36:00  /  2014-11-22 09:36:00\n",
      "2014-12-13 09:36:00  \\  2014-12-18 09:36:00  /  2014-12-20 09:36:00\n",
      "2014-12-20 09:36:00  \\  2014-12-26 09:36:00  /  2014-12-27 09:36:00\n"
     ]
    }
   ],
   "source": [
    "#determine nr of posts in a week\n",
    "\n",
    "first_date = datetime.strptime(\"25/10/2014 - 09:36\", '%d/%m/%Y - %H:%M')\n",
    "last_date = first_date + timedelta(days=366)\n",
    "dates = [first_date + timedelta(days=62),first_date + timedelta(days=23),first_date + timedelta(days=14),first_date + timedelta(days=54)]\n",
    "\n",
    "for d in range(0,(last_date-first_date).days,7):\n",
    "    border1 = first_date+timedelta(days=d)\n",
    "    border2 = first_date+timedelta(days=d+7)\n",
    "    for a_date in dates:\n",
    "        if border1<=a_date<border2:\n",
    "            print border1, \" \\ \", a_date, \" / \", border2\n",
    "            # write to dictionary with borders as keys (start dict at first activity, end at last)\n",
    "\n",
    "# determine average length of values in dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
