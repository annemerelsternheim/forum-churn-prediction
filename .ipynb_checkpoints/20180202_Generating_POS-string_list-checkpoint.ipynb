{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import json, re\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "from pattern.nl import parse, split, parsetree\n",
    "from collections import defaultdict, Counter\n",
    "import unicodedata, unidecode, codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files loaded\n",
      "(2437, 118394)\n"
     ]
    }
   ],
   "source": [
    "# files\n",
    "#MWE\n",
    "#topics = json.load(codecs.open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE_topic.json'))\n",
    "#posts = json.load(codecs.open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\MWE.json'))\n",
    "#regular\n",
    "forums = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-35-45 _amazones_forums_export.json'))\n",
    "topics = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-36-51_amazones_forum_topics_export.json'))\n",
    "posts = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_forum_posts_export.json'))\n",
    "users = json.load(open('D:\\\\4. Data\\\\Amazones_Forum_Export_JSON\\\\2017-12-07T13-39-20_amazones_users_export.json'))\n",
    "print('files loaded')\n",
    "print (len(topics),len(posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# this function replaces ugly links and images by simple, short and readable indicators. It also removes redundant markup from the file (bold, italics, etc).\n",
    "def cleanup(text):\n",
    "    text=re.sub('<a.*?>(.*?)</a>','',text) #remove links\n",
    "    text=re.sub('<img.*?/>', '',text) #remove images\n",
    "    text=re.sub('<div class=\"bb-quote\">((\\s|\\S)*?)</div>','',text) #remove quotes\n",
    "    \n",
    "    #EMOTICONS\n",
    "    text=re.sub('<img.*?title=\"(.*?)\".*?/>', '(EMO:\\\\1)',text) #replace emoticons by textual indicators \n",
    "    text=re.sub(':(\\w+):','(EMO:\\\\1)',text)# replace differently written emoticons by the same textual indicators\n",
    "     \n",
    "    return(re.sub('</?(style|b|p|em|u|i|strong|br|span|div|blockquote|li)(.*?)/?>','',text)) #remove other markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove non-ascii characters from file (otherwise they will become part of the tokens)\n",
    "def remove_non_ascii(text):\n",
    "    return unidecode.unidecode(text)\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    #remove all links, images, quotes, and emailaddresses\n",
    "    text=re.sub('<a.*?>(.*?)</a>','',text) #remove links\n",
    "    text=re.sub('(http:|www)\\S*','',text) #remove links without markup\n",
    "    text=re.sub('\\[\\\\\\/url\\]','',text)\n",
    "    text=re.sub('<img.*?/>', '',text) #remove images\n",
    "    text=re.sub('<div class=\"bb-quote\">((\\s|\\S)*?)</div>','',text) #remove quotes\n",
    "    text=re.sub('<script.*?>([\\S\\s]*?)</script>','',text) #remove emailaddresses\n",
    "\n",
    "    #replace all emoticon-icons\n",
    "    text=re.sub('<img.*?title=\"(.*?)\".*?/>', '(EMO:\\\\1)',text) #replace emoticons by textual indicators \n",
    "\n",
    "    # replace (most) sideways latin emoticons\n",
    "    text=re.sub('[^>]:-?(\\)|\\])','(EMO:smiley)',text)\n",
    "    text=re.sub(u'☺️','(EMO:smiley)',text)\n",
    "    text=re.sub('[^>]:-?(\\(|\\[)','(EMO:sad)',text)\n",
    "    text=re.sub(';-?(\\)|\\])','(EMO:wink)',text)\n",
    "    text=re.sub('(:|;|x|X)-?(D)','(EMO:laugh)',text)\n",
    "    text=re.sub(':-?(/|\\\\\\|\\|)','(EMO:frown)',text)\n",
    "    text=re.sub('(:|;)-?(p|P)','(EMO:cheeky)',text)\n",
    "    text=re.sub('(:|;)(\\'|\\\")-?(\\(|\\[)','(EMO:cry)',text)\n",
    "    text=re.sub('\\<3+','(EMO:heart)',text)\n",
    "    text=re.sub(u'❤️','(EMO:heart)',text)\n",
    "    text=re.sub('((\\>:-?(\\(|\\]))|(\\>?:-?@))','(EMO:angry)',text)\n",
    "    text=re.sub('\\>:-?(\\)|\\])','(EMO:evil)',text)\n",
    "    text=re.sub('(:|;)-?(O|o|0){1}','(EMO:shock)',text)\n",
    "    text=re.sub('(:|;)-?(K|k|x|X)','(EMO:kiss)',text)\n",
    "    \n",
    "    #other important adjustments:\n",
    "    text=re.sub('m\\'?n\\s','mijn ',text) # replacing m'n and mn with mijn, so it gets parsed correctly.\n",
    "    text=re.sub('z\\'?n\\s','zijn ',text) #replacing z'n and zn with zijn\n",
    "    text=re.sub('d\\'?r\\s','haar ',text) #replacing d'r and dr with zijn (only if followed by space, so dr. stays dr.)\n",
    "    \n",
    "    # replace all emoticons (and other things) written between double colons\n",
    "    text=re.sub(':([a-zA-Z]+):','(EMO:\\\\1)',text)\n",
    "\n",
    "    # remove remaining markup\n",
    "    text=re.sub('</?(ol|style|b|p|em|u|i|strong|br|span|div|blockquote|li)(.*?)/?>','',text)\n",
    "    text=re.sub('(\\[|\\]|\\{|\\})', '',text)\n",
    "\n",
    "    # separate text from punctuation (may cause double/triple spaces - does not matter at this point)\n",
    "    text = re.sub('(\\.{2,}|/|\\)|,|!|\\?)','\\\\1 ',text) # space behind\n",
    "    text=re.sub('(/|\\()',' \\\\1',text) # space in front\n",
    "    text=re.sub('(\\w{2,})(\\.|,)','\\\\1 \\\\2 ',text) #space 'between'\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUCK YES: this makes a dictionary with POS tags as keys, and tuples of words and occurrence counts as values (in list)\n",
    "def make_tokentypedict(tokendict):\n",
    "    tokentypedict = defaultdict(list)\n",
    "    for POS in tokendict:\n",
    "        typedict = defaultdict(list)\n",
    "        for token in tokendict[POS]:\n",
    "            typedict[token].append(1)\n",
    "        for types in typedict:\n",
    "            tokentypedict[POS].append((types,len(typedict[types])))\n",
    "    return(tokentypedict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_by_occurence(dictionaryitems):\n",
    "    sortedlist = []\n",
    "    for POS in dictionaryitems:\n",
    "        name=POS[0]\n",
    "        words = POS[1] #list with tuples. This can be sorted\n",
    "        sortedwords = sorted(words, key=lambda tup: tup[1],reverse=True)\n",
    "        newPOS=(name,sortedwords)\n",
    "        sortedlist.append(newPOS)\n",
    "    return sortedlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "# CLEAN UP data and make TOKENDICT (dictionary with POS as keys, and words (in list) as values)\n",
    "# also, make separate dict for emoticons\n",
    "tokendict = defaultdict(list)\n",
    "emolist = []\n",
    "count=0\n",
    "for x in topics:\n",
    "    count+=1\n",
    "    if count%50==0:\n",
    "        print count\n",
    " \n",
    "    s = parse(remove_non_ascii(cleanup(x[\"Body\"])),encoding=\"utf-8\").split()\n",
    "    for sentence in s:\n",
    "        for word in sentence:\n",
    "            if re.search('EMO:',word[0]):\n",
    "                emolist.append(word[0])\n",
    "            elif re.match(\"(N|RB|J|PRP).*?\",word[1]):\n",
    "                tokendict[word[1]].append(word[0].lower())\n",
    "    \n",
    "    for y in reversed(posts):\n",
    "        if y['Forum Topic ID'] == x['Nid']:\n",
    "            s = parse(remove_non_ascii(cleanup(y[\"Body\"])),encoding=\"utf-8\").split()\n",
    "            for sentence in s: \n",
    "                for word in sentence:\n",
    "                    if re.search('EMO:',word[0]):\n",
    "                        emolist.append(word[0])\n",
    "                    elif re.match(\"(N|RB|J|PRP).*?\",word[1]):\n",
    "                        tokendict[word[1]].append(word[0].lower())\n",
    "emodict = Counter(emolist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR QUICK CHECKS IN ALL DATA, or part of it\n",
    "count=0\n",
    "for x in topics:\n",
    "    if count>2250:\n",
    "        break\n",
    "    count+=1\n",
    "    \n",
    "    if count%50==0:\n",
    "        print count\n",
    "        \n",
    "    s = parse(remove_non_ascii(cleanup(x[\"Body\"])),encoding=\"utf-8\").split()\n",
    "    for sentence in s:\n",
    "        #pprint(sentence)\n",
    "        for word in sentence:\n",
    "            #print word\n",
    "            if re.search('^:\\($',word[0]):\n",
    "                pprint(sentence)\n",
    "    for y in reversed(posts):\n",
    "        if y['Forum Topic ID'] == x['Nid']:\n",
    "            s = parse(remove_non_ascii(cleanup(y[\"Body\"])),encoding=\"utf-8\").split()\n",
    "            for sentence in s: \n",
    "                #pprint(sentence)\n",
    "                for word in sentence:\n",
    "                    #print word\n",
    "                    if re.search('^:\\{$',word[0]):\n",
    "                        pprint(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokentypedict = make_tokentypedict(tokendict)\n",
    "#thelist = tokentypedict.items()\n",
    "#pprint(sort_by_occurence(thelist))\n",
    "#pprint(emodict.items())\n",
    "\n",
    "with open('emodict.txt','w') as f:\n",
    "    f.write(json.dumps(emodict))\n",
    "with open('tokentypedict.txt','w') as f:\n",
    "    f.write(json.dumps(tokentypedict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write list to csv. do not open with excel, but import!\n",
    "import csv\n",
    "\n",
    "data=sort_by_occurence(thelist)\n",
    "with open('tokentype.csv','wb') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    for row in data:\n",
    "        pos = row[0]\n",
    "        for types in row:\n",
    "            if types != pos:\n",
    "                for tup in types:\n",
    "                    string=tup[0]\n",
    "                    occ=tup[1]\n",
    "                    csv_out.writerow((pos, string,occ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vooralsnog ben ik blij met deze uitslagen  (EMO:smiley)  . \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"</p><p>Vooralsnog ben ik blij met deze uitslagen ☺️ . </p>\n",
    "\n",
    "<p>\n",
    "\"\"\"\n",
    "print cleanup(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dat is mijn anamnese .  zijn anamnese . .  zijn mijn haar dr . \n"
     ]
    }
   ],
   "source": [
    "text = \"dat is mn anamnese. z'n anamnese.. zn m'n d'r dr.\"\n",
    "print cleanup(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
